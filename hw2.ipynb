{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchmetrics","metadata":{"id":"T0spo_IeBaFQ","outputId":"a4de3fbb-434b-4182-f5c8-a679bb87931a","execution":{"iopub.status.busy":"2021-12-03T15:51:35.990561Z","iopub.execute_input":"2021-12-03T15:51:35.990870Z","iopub.status.idle":"2021-12-03T15:51:45.438362Z","shell.execute_reply.started":"2021-12-03T15:51:35.990839Z","shell.execute_reply":"2021-12-03T15:51:45.437124Z"},"trusted":true},"execution_count":280,"outputs":[]},{"cell_type":"code","source":"!pip install ipdb","metadata":{"id":"t5pIfltMBi8r","outputId":"e2d9aa8d-da94-4537-dfc5-8800cc7abfa3","execution":{"iopub.status.busy":"2021-12-03T15:51:45.442527Z","iopub.execute_input":"2021-12-03T15:51:45.442813Z","iopub.status.idle":"2021-12-03T15:51:54.892790Z","shell.execute_reply.started":"2021-12-03T15:51:45.442779Z","shell.execute_reply":"2021-12-03T15:51:54.891643Z"},"trusted":true},"execution_count":281,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom string import punctuation\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\n\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.nn.utils.rnn import pad_sequence\nfrom gensim.models import FastText\nimport torch.optim as optim\nimport ipdb\nfrom torchmetrics.functional import f1, recall\n","metadata":{"id":"ukkPwgIHBmNc","execution":{"iopub.status.busy":"2021-12-03T15:51:54.896141Z","iopub.execute_input":"2021-12-03T15:51:54.896497Z","iopub.status.idle":"2021-12-03T15:51:54.905868Z","shell.execute_reply.started":"2021-12-03T15:51:54.896440Z","shell.execute_reply":"2021-12-03T15:51:54.904704Z"},"trusted":true},"execution_count":282,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Данные","metadata":{"id":"NINCHuRZCL2X"}},{"cell_type":"code","source":"!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv ","metadata":{"id":"kPkG5C5JBojw","outputId":"eb323cd1-f2de-4d4e-b8a7-dded5f7a79e5","execution":{"iopub.status.busy":"2021-12-03T15:51:54.910281Z","iopub.execute_input":"2021-12-03T15:51:54.911281Z","iopub.status.idle":"2021-12-03T15:51:58.251218Z","shell.execute_reply.started":"2021-12-03T15:51:54.911186Z","shell.execute_reply":"2021-12-03T15:51:58.250071Z"},"trusted":true},"execution_count":283,"outputs":[]},{"cell_type":"code","source":"pos_tweets = pd.read_csv('../input/nlp-tweets/positive.csv', encoding='utf-8', sep=';', header=None,  names=[0,1,2,'text','tone',5,6,7,8,9,10,11])\nneg_tweets = pd.read_csv('../input/nlp-tweets/negative.csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11] )\nneg_tweets['tone'] = 0\nall_tweets_data = pos_tweets.append(neg_tweets)\nprint(len(all_tweets_data))","metadata":{"id":"4hFzpkH1Bqn-","outputId":"d1fa29c0-075a-47d8-82a9-a1b60d0ee6cc","execution":{"iopub.status.busy":"2021-12-03T15:51:58.254100Z","iopub.execute_input":"2021-12-03T15:51:58.254530Z","iopub.status.idle":"2021-12-03T15:51:59.404140Z","shell.execute_reply.started":"2021-12-03T15:51:58.254470Z","shell.execute_reply":"2021-12-03T15:51:59.403161Z"},"trusted":true},"execution_count":284,"outputs":[]},{"cell_type":"code","source":"all_tweets_data","metadata":{"execution":{"iopub.status.busy":"2021-12-03T15:51:59.410235Z","iopub.execute_input":"2021-12-03T15:51:59.413058Z","iopub.status.idle":"2021-12-03T15:51:59.457221Z","shell.execute_reply.started":"2021-12-03T15:51:59.413011Z","shell.execute_reply":"2021-12-03T15:51:59.456312Z"},"trusted":true},"execution_count":285,"outputs":[]},{"cell_type":"code","source":"all_tweets_data = shuffle(all_tweets_data[['text','tone']])[:100000]","metadata":{"id":"S8viTL47BsDL","execution":{"iopub.status.busy":"2021-12-03T15:51:59.468570Z","iopub.execute_input":"2021-12-03T15:51:59.471115Z","iopub.status.idle":"2021-12-03T15:51:59.540455Z","shell.execute_reply.started":"2021-12-03T15:51:59.471068Z","shell.execute_reply":"2021-12-03T15:51:59.538921Z"},"trusted":true},"execution_count":286,"outputs":[]},{"cell_type":"code","source":"def preprocess(text):\n    text = text.lower().replace(\"ё\", \"е\")\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)\n    text = re.sub('@[^\\s]+', 'USER', text)\n    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n    text = re.sub(' +', ' ', text)\n    return text.strip()","metadata":{"id":"t2avfIOrBtkY","execution":{"iopub.status.busy":"2021-12-03T15:51:59.542804Z","iopub.execute_input":"2021-12-03T15:51:59.543527Z","iopub.status.idle":"2021-12-03T15:51:59.559105Z","shell.execute_reply.started":"2021-12-03T15:51:59.543482Z","shell.execute_reply":"2021-12-03T15:51:59.557617Z"},"trusted":true},"execution_count":287,"outputs":[]},{"cell_type":"code","source":"clean_text = all_tweets_data['text'].apply(preprocess)\nall_tweets_data['clean_text'] = clean_text","metadata":{"id":"onwKx7OzBvPI","execution":{"iopub.status.busy":"2021-12-03T15:51:59.561990Z","iopub.execute_input":"2021-12-03T15:51:59.562817Z","iopub.status.idle":"2021-12-03T15:52:02.676740Z","shell.execute_reply.started":"2021-12-03T15:51:59.562767Z","shell.execute_reply":"2021-12-03T15:52:02.675810Z"},"trusted":true},"execution_count":288,"outputs":[]},{"cell_type":"code","source":"all_tweets_data","metadata":{"id":"ELHFPeKhB1aj","outputId":"672ed48f-dfdd-48ff-9996-a8231c43ea9f","execution":{"iopub.status.busy":"2021-12-03T15:52:02.682068Z","iopub.execute_input":"2021-12-03T15:52:02.682351Z","iopub.status.idle":"2021-12-03T15:52:02.697133Z","shell.execute_reply.started":"2021-12-03T15:52:02.682318Z","shell.execute_reply":"2021-12-03T15:52:02.695834Z"},"trusted":true},"execution_count":289,"outputs":[]},{"cell_type":"code","source":"train_sentences, val_sentences = train_test_split(all_tweets_data, test_size=0.1)","metadata":{"id":"6Y4xUh2rB22V","execution":{"iopub.status.busy":"2021-12-03T15:52:02.699415Z","iopub.execute_input":"2021-12-03T15:52:02.699886Z","iopub.status.idle":"2021-12-03T15:52:02.739407Z","shell.execute_reply.started":"2021-12-03T15:52:02.699840Z","shell.execute_reply":"2021-12-03T15:52:02.738318Z"},"trusted":true},"execution_count":290,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDEVICE","metadata":{"id":"i6g_BPioB-R4","outputId":"e5c442f1-6f3b-464b-8232-3c6a1d8d6193","execution":{"iopub.status.busy":"2021-12-03T15:52:02.741158Z","iopub.execute_input":"2021-12-03T15:52:02.741582Z","iopub.status.idle":"2021-12-03T15:52:02.751645Z","shell.execute_reply.started":"2021-12-03T15:52:02.741539Z","shell.execute_reply":"2021-12-03T15:52:02.750288Z"},"trusted":true},"execution_count":291,"outputs":[]},{"cell_type":"markdown","source":"# I","metadata":{"id":"jIux8nwyPgGP"}},{"cell_type":"markdown","source":"## Датасет","metadata":{"id":"b5XW3caCCg20"}},{"cell_type":"code","source":"vocab = Counter()\n\nfor text in all_tweets_data['clean_text']:\n    vocab.update(preprocess(text).split())\nprint('всего уникальных токенов:', len(vocab))","metadata":{"id":"WajBJLtqB4YG","outputId":"7ad6a073-5eda-4688-f964-46d0dac9aa0f","execution":{"iopub.status.busy":"2021-12-03T15:52:02.753454Z","iopub.execute_input":"2021-12-03T15:52:02.754506Z","iopub.status.idle":"2021-12-03T15:52:06.203890Z","shell.execute_reply.started":"2021-12-03T15:52:02.754441Z","shell.execute_reply":"2021-12-03T15:52:06.202672Z"},"trusted":true},"execution_count":292,"outputs":[]},{"cell_type":"code","source":"filtered_vocab = set()\n\nfor word in vocab:\n    if vocab[word] > 2:\n        filtered_vocab.add(word)\nprint('уникальных токенов, втретившихся больше 2 раз:', len(filtered_vocab))","metadata":{"id":"2KqYwhGnB6cp","outputId":"aae4313e-3436-4b51-c72c-c2758f794688","execution":{"iopub.status.busy":"2021-12-03T15:52:06.205820Z","iopub.execute_input":"2021-12-03T15:52:06.207185Z","iopub.status.idle":"2021-12-03T15:52:06.252793Z","shell.execute_reply.started":"2021-12-03T15:52:06.207139Z","shell.execute_reply":"2021-12-03T15:52:06.250881Z"},"trusted":true},"execution_count":293,"outputs":[]},{"cell_type":"code","source":"word2id = {'PAD':0}\n\nfor word in filtered_vocab:\n    word2id[word] = len(word2id)\n\nid2word = {i:word for word, i in word2id.items()}","metadata":{"id":"e8oRWZqcB8m6","execution":{"iopub.status.busy":"2021-12-03T15:52:06.254469Z","iopub.execute_input":"2021-12-03T15:52:06.254892Z","iopub.status.idle":"2021-12-03T15:52:06.277200Z","shell.execute_reply.started":"2021-12-03T15:52:06.254831Z","shell.execute_reply":"2021-12-03T15:52:06.276037Z"},"trusted":true},"execution_count":294,"outputs":[]},{"cell_type":"code","source":"def preprocess(text):\n        text = text.lower().replace(\"ё\", \"е\")\n        text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)\n        text = re.sub('@[^\\s]+', 'USER', text)\n        text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n        text = re.sub(' +', ' ', text)\n        return text.strip()","metadata":{"execution":{"iopub.status.busy":"2021-12-03T15:52:06.279316Z","iopub.execute_input":"2021-12-03T15:52:06.279692Z","iopub.status.idle":"2021-12-03T15:52:06.287573Z","shell.execute_reply.started":"2021-12-03T15:52:06.279627Z","shell.execute_reply":"2021-12-03T15:52:06.286391Z"},"trusted":true},"execution_count":295,"outputs":[]},{"cell_type":"code","source":"preprocess('\";аау\"')","metadata":{"execution":{"iopub.status.busy":"2021-12-03T15:52:06.289429Z","iopub.execute_input":"2021-12-03T15:52:06.289955Z","iopub.status.idle":"2021-12-03T15:52:06.301360Z","shell.execute_reply.started":"2021-12-03T15:52:06.289908Z","shell.execute_reply":"2021-12-03T15:52:06.300167Z"},"trusted":true},"execution_count":296,"outputs":[]},{"cell_type":"code","source":"class TweetsDataset(Dataset):\n\n    def __init__(self, dataset, word2id, DEVICE):\n        self.dataset = dataset['text'].values\n        self.word2id = word2id\n        self.length = dataset.shape[0]\n        self.target = dataset['tone'].values\n        self.device = DEVICE\n\n    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n        return self.length\n\n    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n        tokens = self.preprocess(self.dataset[index]) # токенизируем\n        ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n        y = [self.target[index]]\n        return ids, y\n    \n    def preprocess(self, text):\n        text = text.lower().replace(\"ё\", \"е\")\n        text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)\n        text = re.sub('@[^\\s]+', 'USER', text)\n        text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n        text = re.sub(' +', ' ', text)\n        return text.strip()\n\n    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n    # он понадобится для DataLoader во время итерации по батчам\n      ids, y = list(zip(*batch))\n      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n      #мы хотим применять BCELoss, он будет брать на вход predicted размера batch_size x 1 (так как для каждого семпла модель будет отдавать одно число), target размера batch_size x 1\n      y = torch.Tensor(y).to(self.device) # tuple ([1], [0], [1])  -> Tensor [[1.], [0.], [1.]] \n      return padded_ids, y","metadata":{"id":"Lo0Af1V_CJ0x","execution":{"iopub.status.busy":"2021-12-03T15:52:06.306495Z","iopub.execute_input":"2021-12-03T15:52:06.306790Z","iopub.status.idle":"2021-12-03T15:52:06.319216Z","shell.execute_reply.started":"2021-12-03T15:52:06.306760Z","shell.execute_reply":"2021-12-03T15:52:06.318085Z"},"trusted":true},"execution_count":297,"outputs":[]},{"cell_type":"code","source":"train_dataset = TweetsDataset(train_sentences, word2id, DEVICE)\ntrain_sampler = RandomSampler(train_dataset)\ntrain_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)\nbatch = next(iter(train_iterator))\nbatch[0].shape","metadata":{"id":"zLcXoAb8CVEP","execution":{"iopub.status.busy":"2021-12-03T15:52:06.320893Z","iopub.execute_input":"2021-12-03T15:52:06.321476Z","iopub.status.idle":"2021-12-03T15:52:06.437264Z","shell.execute_reply.started":"2021-12-03T15:52:06.321432Z","shell.execute_reply":"2021-12-03T15:52:06.436363Z"},"trusted":true},"execution_count":298,"outputs":[]},{"cell_type":"code","source":"val_dataset = TweetsDataset(val_sentences, word2id, DEVICE)\nval_sampler = SequentialSampler(val_dataset)\nval_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)","metadata":{"id":"cWvRtVD-FoeU","execution":{"iopub.status.busy":"2021-12-03T15:52:06.440861Z","iopub.execute_input":"2021-12-03T15:52:06.441107Z","iopub.status.idle":"2021-12-03T15:52:06.450195Z","shell.execute_reply.started":"2021-12-03T15:52:06.441078Z","shell.execute_reply":"2021-12-03T15:52:06.448808Z"},"trusted":true},"execution_count":299,"outputs":[]},{"cell_type":"markdown","source":"## Модель","metadata":{"id":"hn44YXZBCrDZ"}},{"cell_type":"code","source":"ft = FastText(all_tweets_data['clean_text'].tolist(), vector_size=100, window=5, min_count=1)\nweights = np.zeros((len(word2id), 100))\ncount = 0\nfor word, i in word2id.items():\n    if word == 'PAD':\n        continue   \n    try:\n        weights[i] = ft.wv[word]    \n    except KeyError:\n      count += 1\n      # oov словам сопоставляем случайный вектор\n      weights[i] = np.random.normal(0,0.1,100)","metadata":{"id":"MRcc6zS_Cspm","execution":{"iopub.status.busy":"2021-12-03T15:52:06.451870Z","iopub.execute_input":"2021-12-03T15:52:06.452625Z","iopub.status.idle":"2021-12-03T15:52:30.697325Z","shell.execute_reply.started":"2021-12-03T15:52:06.452579Z","shell.execute_reply":"2021-12-03T15:52:30.696310Z"},"trusted":true},"execution_count":300,"outputs":[]},{"cell_type":"code","source":"weights = np.zeros((len(word2id), 100))\ncount = 0\nfor word, i in word2id.items():\n    if word == 'PAD':\n        continue   \n    try:\n        weights[i] = ft.wv[word]    \n    except KeyError:\n      count += 1\n      # oov словам сопоставляем случайный вектор\n      weights[i] = np.random.normal(0,0.1,100)","metadata":{"id":"SMe62FXFCxry","execution":{"iopub.status.busy":"2021-12-03T15:52:30.699013Z","iopub.execute_input":"2021-12-03T15:52:30.699419Z","iopub.status.idle":"2021-12-03T15:52:32.701462Z","shell.execute_reply.started":"2021-12-03T15:52:30.699366Z","shell.execute_reply":"2021-12-03T15:52:32.700204Z"},"trusted":true},"execution_count":301,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n    \n    def __init__(self, vocab_size, embedding_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n\n        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n        self.bigrams_over = nn.Conv1d(in_channels=180, out_channels=180, kernel_size=2, padding='same')\n\n        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.relu = nn.ReLU()\n        self.hidden = nn.Linear(in_features=180, out_features=1)\n        self.out = nn.Sigmoid()\n\n    def forward(self, word):\n        #batch_size x seq_len\n        embedded = self.embedding(word)\n        #batch_size x seq_len x embedding_dim\n        embedded = embedded.transpose(1,2)\n        #batch_size x embedding_dim x seq_len\n        feature_map_bigrams = self.relu(self.bigrams(embedded))\n        #batch_size x filter_count2 x seq_len* \n        feature_map_trigrams = self.relu(self.trigrams(embedded))\n        #batch_size x filter_count3 x seq_len*\n\n        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n        bigrams = self.pooling(self.relu(self.bigrams_over(concat)))\n        pooling = bigrams.max(2)[0] \n        # batch _size x (filter_count2 + filter_count3)\n\n        logits = self.hidden(pooling) \n        logits = self.out(logits)      \n        return logits","metadata":{"id":"qXe2hNzLDEBM","execution":{"iopub.status.busy":"2021-12-03T15:52:32.703305Z","iopub.execute_input":"2021-12-03T15:52:32.703839Z","iopub.status.idle":"2021-12-03T15:52:32.725159Z","shell.execute_reply.started":"2021-12-03T15:52:32.703792Z","shell.execute_reply":"2021-12-03T15:52:32.724123Z"},"trusted":true},"execution_count":302,"outputs":[]},{"cell_type":"markdown","source":"## Train loop","metadata":{"id":"3AnpQoOhEySU"}},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion):\n    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n\n    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n\n    for i, (texts, ys) in enumerate(iterator): #итерируемся по батчам\n        optimizer.zero_grad()  #обнуляем градиенты\n        preds = model(texts)  #прогоняем данные через модель\n        loss = criterion(preds, ys) #считаем значение функции потерь  \n        loss.backward() #считаем градиенты  \n        optimizer.step() #обновляем веса \n        epoch_loss += loss.item() #сохраняем значение функции потерь\n        if not (i + 1) % int(len(iterator)/5):\n            print(f'Train loss: {epoch_loss/i}')      \n    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке","metadata":{"id":"r1EqDAPDExYV","execution":{"iopub.status.busy":"2021-12-03T15:52:32.728892Z","iopub.execute_input":"2021-12-03T15:52:32.730735Z","iopub.status.idle":"2021-12-03T15:52:32.746996Z","shell.execute_reply.started":"2021-12-03T15:52:32.730670Z","shell.execute_reply":"2021-12-03T15:52:32.745440Z"},"trusted":true},"execution_count":303,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, iterator, criterion):\n    epoch_loss = 0\n    epoch_metric = 0\n    model.eval() \n    with torch.no_grad():\n        for i, (texts, ys) in enumerate(iterator):   \n            preds = model(texts)  # делаем предсказания на тесте\n            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n            epoch_loss += loss.item()\n            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n            epoch_metric += batch_metric\n\n            if not (i + 1) % int(len(iterator)/5):\n              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n        \n    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке","metadata":{"id":"XdyVb2i4E4Ly","execution":{"iopub.status.busy":"2021-12-03T15:52:32.750243Z","iopub.execute_input":"2021-12-03T15:52:32.753377Z","iopub.status.idle":"2021-12-03T15:52:32.771900Z","shell.execute_reply.started":"2021-12-03T15:52:32.753328Z","shell.execute_reply":"2021-12-03T15:52:32.770625Z"},"trusted":true},"execution_count":304,"outputs":[]},{"cell_type":"markdown","source":"## Обучение\n\nПопробуем для начала стандартные настройки из семинара","metadata":{"id":"xPRk5XqZFUL5"}},{"cell_type":"code","source":"model = CNN(len(word2id), 2)\noptimizer = optim.Adam(model.parameters(), lr=0.0005)\ncriterion = nn.BCELoss()  \n\n# веса модели и значения лосса храним там же, где и все остальные тензоры\nmodel = model.to(DEVICE)\ncriterion = criterion.to(DEVICE)","metadata":{"id":"YWLRmzuxFOQo","execution":{"iopub.status.busy":"2021-12-03T15:52:32.773924Z","iopub.execute_input":"2021-12-03T15:52:32.774608Z","iopub.status.idle":"2021-12-03T15:52:32.806377Z","shell.execute_reply.started":"2021-12-03T15:52:32.774565Z","shell.execute_reply":"2021-12-03T15:52:32.805419Z"},"trusted":true},"execution_count":305,"outputs":[]},{"cell_type":"code","source":"losses = []\nlosses_eval = []\nf1s = []\nf1s_eval = []\n\nfor i in range(20):\n    print(f'\\nstarting Epoch {i}')\n    print('Training...')\n    epoch_loss = train(model, train_iterator, optimizer, criterion)\n    losses.append(epoch_loss)\n    print('\\nEvaluating on train...')\n    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n    f1s.append(f1_on_train)\n    print('\\nEvaluating on test...')\n    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n    losses_eval.append(epoch_loss_on_test)\n    f1s_eval.append(f1_on_test)","metadata":{"id":"vy0Z8yvmFe1U","execution":{"iopub.status.busy":"2021-12-03T15:52:32.812060Z","iopub.execute_input":"2021-12-03T15:52:32.814866Z","iopub.status.idle":"2021-12-03T15:57:52.406482Z","shell.execute_reply.started":"2021-12-03T15:52:32.814783Z","shell.execute_reply":"2021-12-03T15:57:52.405281Z"},"trusted":true},"execution_count":306,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 6))\nplt.title('Train')\nplt.xlabel('Iterations')\nplt.ylabel('Losses')\nplt.grid()\nax.plot(losses, label='Trainig loss')\nax.plot(losses_eval, label='Evaluation loss')\nax.legend()\nplt.show()","metadata":{"id":"1sKoa5VKJ4T6","execution":{"iopub.status.busy":"2021-12-03T15:57:52.415405Z","iopub.execute_input":"2021-12-03T15:57:52.415670Z","iopub.status.idle":"2021-12-03T15:57:52.703288Z","shell.execute_reply.started":"2021-12-03T15:57:52.415623Z","shell.execute_reply":"2021-12-03T15:57:52.702163Z"},"trusted":true},"execution_count":307,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 6))\nplt.title('Train')\nplt.xlabel('Iterations')\nplt.ylabel('Losses')\nplt.grid()\nax.plot(f1s, label='Train F1')\nax.plot(f1s_eval, label='Evaluation F1')\nax.legend()\nplt.show()","metadata":{"id":"dNG9vP5TOYQ-","execution":{"iopub.status.busy":"2021-12-03T15:57:52.705030Z","iopub.execute_input":"2021-12-03T15:57:52.705414Z","iopub.status.idle":"2021-12-03T15:57:52.992515Z","shell.execute_reply.started":"2021-12-03T15:57:52.705370Z","shell.execute_reply":"2021-12-03T15:57:52.991379Z"},"trusted":true},"execution_count":308,"outputs":[]},{"cell_type":"code","source":"print('F1 training score: ', f1s[-1])\nprint('F1 evaluation score: ', f1s_eval[-1])\nprint('Training loss: ',losses[-1])\nprint('Evaluation loss: ',losses_eval[-1])","metadata":{"id":"onxog3rQ7qXy","execution":{"iopub.status.busy":"2021-12-03T15:57:52.994099Z","iopub.execute_input":"2021-12-03T15:57:52.995403Z","iopub.status.idle":"2021-12-03T15:57:53.008854Z","shell.execute_reply.started":"2021-12-03T15:57:52.995350Z","shell.execute_reply":"2021-12-03T15:57:53.007456Z"},"trusted":true},"execution_count":309,"outputs":[]},{"cell_type":"code","source":"def predict(model, iterator):\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for i, (words, ys) in enumerate(iterator): \n            for word in model(words):\n                preds.append(word.cpu().detach().numpy().round())  # делаем предсказания на тесте \n    return preds","metadata":{"execution":{"iopub.status.busy":"2021-12-03T15:57:53.011332Z","iopub.execute_input":"2021-12-03T15:57:53.011690Z","iopub.status.idle":"2021-12-03T15:57:53.019059Z","shell.execute_reply.started":"2021-12-03T15:57:53.011644Z","shell.execute_reply":"2021-12-03T15:57:53.017934Z"},"trusted":true},"execution_count":310,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T15:57:53.020710Z","iopub.execute_input":"2021-12-03T15:57:53.022475Z","iopub.status.idle":"2021-12-03T15:57:53.030176Z","shell.execute_reply.started":"2021-12-03T15:57:53.022343Z","shell.execute_reply":"2021-12-03T15:57:53.028969Z"},"trusted":true},"execution_count":311,"outputs":[]},{"cell_type":"code","source":"val_sentences['predicted']  = predict(model, val_iterator)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T15:57:53.034125Z","iopub.execute_input":"2021-12-03T15:57:53.034432Z","iopub.status.idle":"2021-12-03T15:57:54.357008Z","shell.execute_reply.started":"2021-12-03T15:57:53.034393Z","shell.execute_reply":"2021-12-03T15:57:54.354527Z"},"trusted":true},"execution_count":312,"outputs":[]},{"cell_type":"code","source":"# TP\nval_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 1)][['clean_text']]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T15:57:54.358698Z","iopub.execute_input":"2021-12-03T15:57:54.358985Z","iopub.status.idle":"2021-12-03T15:57:54.400350Z","shell.execute_reply.started":"2021-12-03T15:57:54.358942Z","shell.execute_reply":"2021-12-03T15:57:54.399290Z"},"trusted":true},"execution_count":313,"outputs":[]},{"cell_type":"code","source":"# FN\nval_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 0)][['clean_text']]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T15:57:54.402001Z","iopub.execute_input":"2021-12-03T15:57:54.402379Z","iopub.status.idle":"2021-12-03T15:57:54.440179Z","shell.execute_reply.started":"2021-12-03T15:57:54.402305Z","shell.execute_reply":"2021-12-03T15:57:54.438929Z"},"trusted":true},"execution_count":314,"outputs":[]},{"cell_type":"code","source":"# FP\nval_sentences[(val_sentences['tone'] == 0) & (val_sentences['predicted'] == 1)][['clean_text']]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T15:57:54.441608Z","iopub.execute_input":"2021-12-03T15:57:54.442272Z","iopub.status.idle":"2021-12-03T15:57:54.479551Z","shell.execute_reply.started":"2021-12-03T15:57:54.442197Z","shell.execute_reply":"2021-12-03T15:57:54.478450Z"},"trusted":true},"execution_count":315,"outputs":[]},{"cell_type":"markdown","source":"Результаты страннные, но можно заметить что в FN чаще есть сообщения со словом \"не\" или плохими словами\nв FP не распознается сарказм (но это большая придирка, это делать сложно) USER меня ща все заанфолловят наверн\n\nТак же: почему именно последние дни всегда самые лучшие - тожже FP, видимо из-за слова \"лучший\"","metadata":{}},{"cell_type":"markdown","source":"## Улучшение\n\nВ архитектуру нейросети добавим dropout=0.5","metadata":{"id":"rnQ3KvtYpR7-"}},{"cell_type":"code","source":"class CNNDropout(nn.Module):\n    \n    def __init__(self, vocab_size, embedding_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n\n        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n        self.bigrams_over = nn.Conv1d(in_channels=180, out_channels=180, kernel_size=2, padding='same')\n\n        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.relu = nn.ReLU()\n        self.hidden = nn.Linear(in_features=180, out_features=1)\n        self.dropout = nn.Dropout(p=0.5)\n        self.out = nn.Sigmoid()\n\n    def forward(self, word):\n        #batch_size x seq_len\n        embedded = self.embedding(word)\n        #batch_size x seq_len x embedding_dim\n        embedded = embedded.transpose(1,2)\n        #batch_size x embedding_dim x seq_len\n        feature_map_bigrams = self.relu(self.bigrams(embedded))\n        #batch_size x filter_count2 x seq_len* \n        feature_map_trigrams = self.relu(self.trigrams(embedded))\n        #batch_size x filter_count3 x seq_len*\n\n        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n        bigrams = self.dropout(self.pooling(self.relu(self.bigrams_over(concat))))\n        pooling = bigrams.max(2)[0] \n        # batch _size x (filter_count2 + filter_count3)\n\n        logits = self.hidden(pooling)\n        logits = self.out(logits)      \n        return logits","metadata":{"id":"HXSzZBBv7Npe","execution":{"iopub.status.busy":"2021-12-03T15:59:30.614366Z","iopub.execute_input":"2021-12-03T15:59:30.614752Z","iopub.status.idle":"2021-12-03T15:59:30.630211Z","shell.execute_reply.started":"2021-12-03T15:59:30.614719Z","shell.execute_reply":"2021-12-03T15:59:30.629039Z"},"trusted":true},"execution_count":317,"outputs":[]},{"cell_type":"markdown","source":"Попробуем поднять размерность эмбеддингов до 30, увеличить частоту обучения и добавить L2 регуляризацию\nСтандартное значение 1e-2, но я уменьшил его до 1е-4 чтобы не так сильно штрафовать веса\nТакже увеличим количество эпох","metadata":{}},{"cell_type":"code","source":"model = CNNDropout(len(word2id), 30)\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\ncriterion = nn.BCELoss()  \n\n# веса модели и значения лосса храним там же, где и все остальные тензоры\nmodel = model.to(DEVICE)\ncriterion = criterion.to(DEVICE)","metadata":{"id":"Trzan4vy8do0","execution":{"iopub.status.busy":"2021-12-03T15:59:31.835539Z","iopub.execute_input":"2021-12-03T15:59:31.835825Z","iopub.status.idle":"2021-12-03T15:59:31.861812Z","shell.execute_reply.started":"2021-12-03T15:59:31.835793Z","shell.execute_reply":"2021-12-03T15:59:31.860868Z"},"trusted":true},"execution_count":318,"outputs":[]},{"cell_type":"code","source":"losses = []\nlosses_eval = []\nf1s = []\nf1s_eval = []\n\nfor i in range(20):\n    print(f'\\nstarting Epoch {i}')\n    print('Training...')\n    epoch_loss = train(model, train_iterator, optimizer, criterion)\n    losses.append(epoch_loss)\n    print('\\nEvaluating on train...')\n    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n    f1s.append(f1_on_train)\n    print('\\nEvaluating on test...')\n    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n    losses_eval.append(epoch_loss_on_test)\n    f1s_eval.append(f1_on_test)","metadata":{"id":"oIx1Jiw58oNU","execution":{"iopub.status.busy":"2021-12-03T15:59:32.635394Z","iopub.execute_input":"2021-12-03T15:59:32.635689Z","iopub.status.idle":"2021-12-03T16:05:03.465204Z","shell.execute_reply.started":"2021-12-03T15:59:32.635656Z","shell.execute_reply":"2021-12-03T16:05:03.464190Z"},"trusted":true},"execution_count":319,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 6))\nplt.title('Train')\nplt.xlabel('Iterations')\nplt.ylabel('Losses')\nplt.grid()\nax.plot(losses, label='Trainig loss')\nax.plot(losses_eval, label='Evaluation loss')\nax.legend()\nplt.show()","metadata":{"id":"pWFUz0E-8sdw","execution":{"iopub.status.busy":"2021-12-03T16:05:03.467428Z","iopub.execute_input":"2021-12-03T16:05:03.467928Z","iopub.status.idle":"2021-12-03T16:05:03.753156Z","shell.execute_reply.started":"2021-12-03T16:05:03.467881Z","shell.execute_reply":"2021-12-03T16:05:03.752284Z"},"trusted":true},"execution_count":320,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 6))\nplt.title('Train')\nplt.xlabel('Iterations')\nplt.ylabel('Losses')\nplt.grid()\nax.plot(f1s, label='Train F1')\nax.plot(f1s_eval, label='Evaluation F1')\nax.legend()\nplt.show()","metadata":{"id":"r8KiDSfM8wXP","execution":{"iopub.status.busy":"2021-12-03T16:05:03.754503Z","iopub.execute_input":"2021-12-03T16:05:03.754805Z","iopub.status.idle":"2021-12-03T16:05:04.029499Z","shell.execute_reply.started":"2021-12-03T16:05:03.754765Z","shell.execute_reply":"2021-12-03T16:05:04.028287Z"},"trusted":true},"execution_count":321,"outputs":[]},{"cell_type":"code","source":"print('F1 training score: ', f1s[-1])\nprint('F1 evaluation score: ', f1s_eval[-1])\nprint('Training loss: ',losses[-1])\nprint('Evaluation loss: ',losses_eval[-1])","metadata":{"id":"0r9KfOyX8y76","execution":{"iopub.status.busy":"2021-12-03T16:05:04.032261Z","iopub.execute_input":"2021-12-03T16:05:04.033017Z","iopub.status.idle":"2021-12-03T16:05:04.047402Z","shell.execute_reply.started":"2021-12-03T16:05:04.032956Z","shell.execute_reply":"2021-12-03T16:05:04.045711Z"},"trusted":true},"execution_count":322,"outputs":[]},{"cell_type":"markdown","source":"Метрики улучшились, посмотрим теперь примеры","metadata":{}},{"cell_type":"code","source":"val_sentences['predicted']  = predict(model, val_iterator)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T16:05:04.049007Z","iopub.execute_input":"2021-12-03T16:05:04.049882Z","iopub.status.idle":"2021-12-03T16:05:05.145766Z","shell.execute_reply.started":"2021-12-03T16:05:04.049831Z","shell.execute_reply":"2021-12-03T16:05:05.143379Z"},"trusted":true},"execution_count":323,"outputs":[]},{"cell_type":"code","source":"# TP\nval_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 1)][['clean_text']]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T16:05:05.147084Z","iopub.execute_input":"2021-12-03T16:05:05.147438Z","iopub.status.idle":"2021-12-03T16:05:05.189917Z","shell.execute_reply.started":"2021-12-03T16:05:05.147373Z","shell.execute_reply":"2021-12-03T16:05:05.188758Z"},"trusted":true},"execution_count":324,"outputs":[]},{"cell_type":"code","source":"# FN\nval_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 0)][['clean_text']]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T16:05:05.192125Z","iopub.execute_input":"2021-12-03T16:05:05.192702Z","iopub.status.idle":"2021-12-03T16:05:05.231249Z","shell.execute_reply.started":"2021-12-03T16:05:05.192655Z","shell.execute_reply":"2021-12-03T16:05:05.230131Z"},"trusted":true},"execution_count":325,"outputs":[]},{"cell_type":"code","source":"# FP\nval_sentences[(val_sentences['tone'] == 0) & (val_sentences['predicted'] == 1)][['clean_text']]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T16:07:20.716415Z","iopub.execute_input":"2021-12-03T16:07:20.716712Z","iopub.status.idle":"2021-12-03T16:07:20.757391Z","shell.execute_reply.started":"2021-12-03T16:07:20.716681Z","shell.execute_reply":"2021-12-03T16:07:20.756230Z"},"trusted":true},"execution_count":326,"outputs":[]},{"cell_type":"markdown","source":"Результаты не сильно изменились, но метрики тожже не сильно улучшились","metadata":{}},{"cell_type":"markdown","source":"# II","metadata":{"id":"XZwt4sNfPn0U"}},{"cell_type":"markdown","source":"## Датасет","metadata":{"id":"Qgb9DVUxQ5I3"}},{"cell_type":"code","source":"vocab = Counter()\n\nfor text in all_tweets_data['clean_text']:\n    vocab.update(preprocess(text).split())\nprint('всего уникальных токенов:', len(vocab))","metadata":{"id":"vZSmHcpsQ_V-","execution":{"iopub.status.busy":"2021-12-03T16:07:27.073472Z","iopub.execute_input":"2021-12-03T16:07:27.073932Z","iopub.status.idle":"2021-12-03T16:07:30.593271Z","shell.execute_reply.started":"2021-12-03T16:07:27.073896Z","shell.execute_reply":"2021-12-03T16:07:30.592050Z"},"trusted":true},"execution_count":327,"outputs":[]},{"cell_type":"code","source":"filtered_vocab = set()\n\nfor word in vocab:\n    if vocab[word] > 2:\n        filtered_vocab.add(word)\nprint('уникальных токенов, втретившихся больше 2 раз:', len(filtered_vocab))","metadata":{"id":"Hw0wdwam9uvT","execution":{"iopub.status.busy":"2021-12-03T16:07:30.595824Z","iopub.execute_input":"2021-12-03T16:07:30.596442Z","iopub.status.idle":"2021-12-03T16:07:30.643926Z","shell.execute_reply.started":"2021-12-03T16:07:30.596338Z","shell.execute_reply":"2021-12-03T16:07:30.642657Z"},"trusted":true},"execution_count":328,"outputs":[]},{"cell_type":"code","source":"word2id = {'PAD':0}\n\nfor word in filtered_vocab:\n    word2id[word] = len(word2id)\n\nid2word = {i:word for word, i in word2id.items()}","metadata":{"id":"b7dc70ufROpA","execution":{"iopub.status.busy":"2021-12-03T16:07:30.646712Z","iopub.execute_input":"2021-12-03T16:07:30.647088Z","iopub.status.idle":"2021-12-03T16:07:30.670097Z","shell.execute_reply.started":"2021-12-03T16:07:30.647043Z","shell.execute_reply":"2021-12-03T16:07:30.669150Z"},"trusted":true},"execution_count":329,"outputs":[]},{"cell_type":"code","source":"symbol_vocab = Counter()\nfor text in all_tweets_data['clean_text']:\n    symbol_vocab.update(list(text))\nprint('всего уникальных символов:', len(symbol_vocab))","metadata":{"id":"AONF0taoRuuR","execution":{"iopub.status.busy":"2021-12-03T16:07:32.067998Z","iopub.execute_input":"2021-12-03T16:07:32.068678Z","iopub.status.idle":"2021-12-03T16:07:33.266374Z","shell.execute_reply.started":"2021-12-03T16:07:32.068627Z","shell.execute_reply":"2021-12-03T16:07:33.265336Z"},"trusted":true},"execution_count":330,"outputs":[]},{"cell_type":"code","source":"symbol2id = {'PAD':0}\n\nfor symbol in symbol_vocab:\n    symbol2id[symbol] = len(symbol2id)\n\nid2symbol = {i:symbol for symbol, i in symbol2id.items()}","metadata":{"id":"H6PYQ_2cRn4s","execution":{"iopub.status.busy":"2021-12-03T16:07:34.488138Z","iopub.execute_input":"2021-12-03T16:07:34.488796Z","iopub.status.idle":"2021-12-03T16:07:34.495904Z","shell.execute_reply.started":"2021-12-03T16:07:34.488761Z","shell.execute_reply":"2021-12-03T16:07:34.493465Z"},"trusted":true},"execution_count":331,"outputs":[]},{"cell_type":"markdown","source":"Добавим символы в датасет","metadata":{}},{"cell_type":"code","source":"class TweetsDatasetWordSymbol(Dataset):\n\n    def __init__(self, dataset, word2id, symbol2id, DEVICE):\n        self.dataset = dataset['clean_text'].values\n        self.word2id = word2id\n        self.symbol2id = symbol2id\n\n        self.length = dataset.shape[0]\n        self.target = torch.Tensor(dataset['tone'].values)\n        self.device = DEVICE\n\n    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n        return self.length\n\n    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n        symbols = list(self.dataset[index])\n        symbol_ids = torch.LongTensor([self.symbol2id[symbol] for symbol in symbols if symbol in self.symbol2id])\n        tokens = self.dataset[index].split()\n        word_ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n        y = [self.target[index]]\n        return word_ids, symbol_ids, y\n\n    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n    # он понадобится для DataLoader во время итерации по батчам\n      word_ids, symbol_ids, y = list(zip(*batch))\n      padded_words = pad_sequence(word_ids, batch_first=True).to(self.device)\n      padded_symbols = pad_sequence(symbol_ids, batch_first=True).to(self.device)\n      #мы хотим применять BCELoss, он будет брать на вход predicted размера batch_size x 1 (так как для каждого семпла модель будет отдавать одно число), target размера batch_size x 1 \n      y = torch.Tensor(y).to(self.device) # tuple ([1], [0], [1])  -> Tensor [[1.], [0.], [1.]]\n      return padded_words, padded_symbols, y","metadata":{"id":"02hWTNQhTmGR","execution":{"iopub.status.busy":"2021-12-03T16:07:37.374330Z","iopub.execute_input":"2021-12-03T16:07:37.374681Z","iopub.status.idle":"2021-12-03T16:07:37.390195Z","shell.execute_reply.started":"2021-12-03T16:07:37.374638Z","shell.execute_reply":"2021-12-03T16:07:37.388948Z"},"trusted":true},"execution_count":332,"outputs":[]},{"cell_type":"code","source":"train_dataset = TweetsDatasetWordSymbol(train_sentences, word2id, symbol2id, DEVICE)\ntrain_sampler = RandomSampler(train_dataset)\ntrain_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)\nbatch = next(iter(train_iterator))\nbatch[0].shape","metadata":{"id":"ViooHxb4W-__","execution":{"iopub.status.busy":"2021-12-03T16:07:40.842677Z","iopub.execute_input":"2021-12-03T16:07:40.843488Z","iopub.status.idle":"2021-12-03T16:07:40.933164Z","shell.execute_reply.started":"2021-12-03T16:07:40.843439Z","shell.execute_reply":"2021-12-03T16:07:40.931347Z"},"trusted":true},"execution_count":333,"outputs":[]},{"cell_type":"code","source":"val_dataset = TweetsDatasetWordSymbol(val_sentences, word2id, symbol2id, DEVICE)\nval_sampler = SequentialSampler(val_dataset)\nval_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)","metadata":{"id":"pDD9PEQtXEVV","execution":{"iopub.status.busy":"2021-12-03T16:07:41.698439Z","iopub.execute_input":"2021-12-03T16:07:41.699350Z","iopub.status.idle":"2021-12-03T16:07:41.709478Z","shell.execute_reply.started":"2021-12-03T16:07:41.699314Z","shell.execute_reply":"2021-12-03T16:07:41.708409Z"},"trusted":true},"execution_count":334,"outputs":[]},{"cell_type":"markdown","source":"## Модель","metadata":{"id":"1DLCaOQ9Pq5c"}},{"cell_type":"markdown","source":"Сразу добавим дропаут в архитектуру","metadata":{}},{"cell_type":"code","source":"class CNNWordSymbol(nn.Module):\n    def __init__(self, word_vocab_size, symbol_vocab_size, symbol_emb_dim):\n        super().__init__()\n\n        self.word_embedding = nn.Embedding(word_vocab_size, 100)\n        self.word_embedding.from_pretrained(torch.tensor(weights), freeze=True)\n        self.symbol_embedding = nn.Embedding(symbol_vocab_size, symbol_emb_dim)\n        self.symbol_bigrams = nn.Conv1d(in_channels=symbol_emb_dim, out_channels=100, kernel_size=2, padding='same')\n        self.symbol_trigrams = nn.Conv1d(in_channels=symbol_emb_dim, out_channels=80, kernel_size=3, padding='same')\n        self.symbol_pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.word_hidden = nn.Linear(100, 100)\n        self.symbol_hidden = nn.Linear(in_features=180, out_features=100)\n        self.linear = nn.Linear(in_features=200, out_features=1)\n        self.relu = nn.ReLU()   \n        self.dropout = nn.Dropout(p=0.5)\n        self.out = nn.Sigmoid()\n\n\n    def forward(self, word_seq, symbol_seq):\n        #batch_size x seq_len\n        embedded = self.symbol_embedding(symbol_seq)\n        #batch_size x seq_len x embedding_dim\n        embedded = embedded.transpose(1,2)\n        #batch_size x embedding_dim x seq_len\n        feature_map_bigrams = self.symbol_pooling(self.relu(self.symbol_bigrams(embedded)))\n        #batch_size x filter_count2 x seq_len* \n        feature_map_trigrams = self.symbol_pooling(self.relu(self.symbol_trigrams(embedded)))\n        #batch_size x filter_count3 x seq_len*\n\n        pooling1 = feature_map_bigrams.max(2)[0] \n        # batch_size x filter_count2\n        pooling2 = feature_map_trigrams.max(2)[0]\n        # batch_size x filter_count3\n        concat = torch.cat((pooling1, pooling2), 1)\n        # batch_size x (filter_count2 + filter_count3)\n        symbol_emb = self.symbol_hidden(concat)\n\n        embedded_words = self.word_embedding(word_seq)\n        mean_emb_words = torch.mean(embedded_words, dim=1)\n        X = self.dropout(self.word_hidden(mean_emb_words)) \n        X = self.dropout(self.relu(X))\n        concat = torch.cat((symbol_emb, X), 1)\n        logits = self.out(self.linear(concat))      \n        return logits","metadata":{"id":"mYRTK2eWPpSR","execution":{"iopub.status.busy":"2021-12-03T16:08:06.040346Z","iopub.execute_input":"2021-12-03T16:08:06.040640Z","iopub.status.idle":"2021-12-03T16:08:06.055879Z","shell.execute_reply.started":"2021-12-03T16:08:06.040610Z","shell.execute_reply":"2021-12-03T16:08:06.054439Z"},"trusted":true},"execution_count":335,"outputs":[]},{"cell_type":"markdown","source":"## Train loop","metadata":{"id":"ZYQRUrSIVhQ3"}},{"cell_type":"markdown","source":"Сделаем новые обучалки, чтобы учитывать символы","metadata":{}},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion):\n    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n\n    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n\n    for i, (texts, symbols, ys) in enumerate(iterator): #итерируемся по батчам\n        optimizer.zero_grad()  #обнуляем градиенты\n        preds = model(texts, symbols)  #прогоняем данные через модель\n        loss = criterion(preds, ys) #считаем значение функции потерь  \n        loss.backward() #считаем градиенты  \n        optimizer.step() #обновляем веса \n        epoch_loss += loss.item() #сохраняем значение функции потерь\n        if not (i + 1) % int(len(iterator)/5):\n            print(f'Train loss: {epoch_loss/i}')      \n    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке","metadata":{"id":"G79WcsWzVl97","execution":{"iopub.status.busy":"2021-12-03T16:08:07.718062Z","iopub.execute_input":"2021-12-03T16:08:07.719421Z","iopub.status.idle":"2021-12-03T16:08:07.728444Z","shell.execute_reply.started":"2021-12-03T16:08:07.719369Z","shell.execute_reply":"2021-12-03T16:08:07.727414Z"},"trusted":true},"execution_count":336,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, iterator, criterion):\n    epoch_loss = 0\n    epoch_metric = 0\n    model.eval() \n    with torch.no_grad():\n        for i, (texts, symbols, ys) in enumerate(iterator):   \n            preds = model(texts, symbols)  # делаем предсказания на тесте\n            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n            epoch_loss += loss.item()\n            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n            epoch_metric += batch_metric\n\n            if not (i + 1) % int(len(iterator)/5):\n              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n        \n    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке","metadata":{"id":"Kffm6MzrVpni","execution":{"iopub.status.busy":"2021-12-03T16:08:09.809670Z","iopub.execute_input":"2021-12-03T16:08:09.810566Z","iopub.status.idle":"2021-12-03T16:08:09.818108Z","shell.execute_reply.started":"2021-12-03T16:08:09.810532Z","shell.execute_reply":"2021-12-03T16:08:09.817150Z"},"trusted":true},"execution_count":337,"outputs":[]},{"cell_type":"markdown","source":"## Обучение","metadata":{"id":"j-j7fptIVqnj"}},{"cell_type":"markdown","source":"Поставим такую же размерность эмбеддингов, частоту обучения и регуляризацию","metadata":{}},{"cell_type":"code","source":"model = CNNWordSymbol(len(word2id), len(symbol2id), 14)\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\ncriterion = nn.BCELoss()  \n\n# веса модели и значения лосса храним там же, где и все остальные тензоры\nmodel = model.to(DEVICE)\ncriterion = criterion.to(DEVICE)","metadata":{"id":"65fLKUURVsw2","execution":{"iopub.status.busy":"2021-12-03T16:08:15.074953Z","iopub.execute_input":"2021-12-03T16:08:15.075347Z","iopub.status.idle":"2021-12-03T16:08:15.121679Z","shell.execute_reply.started":"2021-12-03T16:08:15.075301Z","shell.execute_reply":"2021-12-03T16:08:15.120696Z"},"trusted":true},"execution_count":338,"outputs":[]},{"cell_type":"code","source":"losses = []\nlosses_eval = []\nf1s = []\nf1s_eval = []\n\nfor i in range(20):\n    print(f'\\nstarting Epoch {i}')\n    print('Training...')\n    epoch_loss = train(model, train_iterator, optimizer, criterion)\n    losses.append(epoch_loss)\n    print('\\nEvaluating on train...')\n    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n    f1s.append(f1_on_train)\n    print('\\nEvaluating on test...')\n    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n    losses_eval.append(epoch_loss_on_test)\n    f1s_eval.append(f1_on_test)","metadata":{"id":"oZV0kk2cVu4h","execution":{"iopub.status.busy":"2021-12-03T16:08:23.211909Z","iopub.execute_input":"2021-12-03T16:08:23.212245Z","iopub.status.idle":"2021-12-03T16:12:46.880732Z","shell.execute_reply.started":"2021-12-03T16:08:23.212200Z","shell.execute_reply":"2021-12-03T16:12:46.879647Z"},"trusted":true},"execution_count":340,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 6))\nplt.title('Train')\nplt.xlabel('Iterations')\nplt.ylabel('Losses')\nplt.grid()\nax.plot(losses, label='Trainig loss')\nax.plot(losses_eval, label='Evaluation loss')\nax.legend()\nplt.show()","metadata":{"id":"FDhZWVdKWICk","execution":{"iopub.status.busy":"2021-12-03T16:12:46.883077Z","iopub.execute_input":"2021-12-03T16:12:46.883508Z","iopub.status.idle":"2021-12-03T16:12:47.160827Z","shell.execute_reply.started":"2021-12-03T16:12:46.883462Z","shell.execute_reply":"2021-12-03T16:12:47.159704Z"},"trusted":true},"execution_count":341,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 6))\nplt.title('Train')\nplt.xlabel('Iterations')\nplt.ylabel('Losses')\nplt.grid()\nax.plot(f1s, label='Train F1')\nax.plot(f1s_eval, label='Evaluation F1')\nax.legend()\nplt.show()","metadata":{"id":"xAnVMVyRc87O","execution":{"iopub.status.busy":"2021-12-03T16:12:47.162443Z","iopub.execute_input":"2021-12-03T16:12:47.164431Z","iopub.status.idle":"2021-12-03T16:12:47.450922Z","shell.execute_reply.started":"2021-12-03T16:12:47.164383Z","shell.execute_reply":"2021-12-03T16:12:47.449964Z"},"trusted":true},"execution_count":342,"outputs":[]},{"cell_type":"code","source":"print('F1 training score: ', f1s[-1])\nprint('F1 evaluation score: ', f1s_eval[-1])\nprint('Training loss: ',losses[-1])\nprint('Evaluation loss: ',losses_eval[-1])","metadata":{"id":"VoTe4RX_fdQF","execution":{"iopub.status.busy":"2021-12-03T16:12:47.454258Z","iopub.execute_input":"2021-12-03T16:12:47.454655Z","iopub.status.idle":"2021-12-03T16:12:47.468677Z","shell.execute_reply.started":"2021-12-03T16:12:47.454593Z","shell.execute_reply":"2021-12-03T16:12:47.467515Z"},"trusted":true},"execution_count":343,"outputs":[]},{"cell_type":"code","source":"def predict_word_symbol(model, iterator):\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for i, (words, symbols, ys) in enumerate(iterator): \n            for word in model(words, symbols):\n                preds.append(word.cpu().detach().numpy().round())  # делаем предсказания на тесте \n    return preds","metadata":{"execution":{"iopub.status.busy":"2021-12-03T16:18:13.828630Z","iopub.execute_input":"2021-12-03T16:18:13.829128Z","iopub.status.idle":"2021-12-03T16:18:13.839557Z","shell.execute_reply.started":"2021-12-03T16:18:13.829079Z","shell.execute_reply":"2021-12-03T16:18:13.838177Z"},"trusted":true},"execution_count":348,"outputs":[]},{"cell_type":"code","source":"val_sentences['predicted']  = predict_word_symbol(model, val_iterator)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T16:18:14.628297Z","iopub.execute_input":"2021-12-03T16:18:14.628886Z","iopub.status.idle":"2021-12-03T16:18:15.557926Z","shell.execute_reply.started":"2021-12-03T16:18:14.628851Z","shell.execute_reply":"2021-12-03T16:18:15.555409Z"},"trusted":true},"execution_count":349,"outputs":[]},{"cell_type":"code","source":"# TP\nval_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 1)][['clean_text']]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T16:18:18.662393Z","iopub.execute_input":"2021-12-03T16:18:18.662683Z","iopub.status.idle":"2021-12-03T16:18:18.705571Z","shell.execute_reply.started":"2021-12-03T16:18:18.662654Z","shell.execute_reply":"2021-12-03T16:18:18.704622Z"},"trusted":true},"execution_count":350,"outputs":[]},{"cell_type":"code","source":"# FN\nval_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 0)][['clean_text']]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T16:18:19.026141Z","iopub.execute_input":"2021-12-03T16:18:19.026426Z","iopub.status.idle":"2021-12-03T16:18:19.063984Z","shell.execute_reply.started":"2021-12-03T16:18:19.026395Z","shell.execute_reply":"2021-12-03T16:18:19.062702Z"},"trusted":true},"execution_count":351,"outputs":[]},{"cell_type":"code","source":"# FP\nval_sentences[(val_sentences['tone'] == 0) & (val_sentences['predicted'] == 1)][['clean_text']]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T16:18:19.642568Z","iopub.execute_input":"2021-12-03T16:18:19.642854Z","iopub.status.idle":"2021-12-03T16:18:19.681614Z","shell.execute_reply.started":"2021-12-03T16:18:19.642823Z","shell.execute_reply":"2021-12-03T16:18:19.680311Z"},"trusted":true},"execution_count":352,"outputs":[]},{"cell_type":"markdown","source":"По сравнению с предыдущими моделями, сильных измененний в результатах нет, но с другой стороны их нет, потому что мы убираем много символов в предобработке","metadata":{}},{"cell_type":"markdown","source":"## Улучшение","metadata":{"id":"7QLXHCRZ86by"}},{"cell_type":"markdown","source":"Попробуем улучшить модель, не делая предобработку","metadata":{}},{"cell_type":"code","source":"vocab = Counter()\n\nfor text in all_tweets_data['text']:\n    vocab.update(text.split())\nprint('всего уникальных токенов:', len(vocab))","metadata":{"id":"8hAM9mdm9a2G","execution":{"iopub.status.busy":"2021-12-03T16:18:36.090033Z","iopub.execute_input":"2021-12-03T16:18:36.090358Z","iopub.status.idle":"2021-12-03T16:18:36.760304Z","shell.execute_reply.started":"2021-12-03T16:18:36.090326Z","shell.execute_reply":"2021-12-03T16:18:36.757981Z"},"trusted":true},"execution_count":353,"outputs":[]},{"cell_type":"code","source":"filtered_vocab = set()\n\nfor word in vocab:\n    if vocab[word] > 2:\n        filtered_vocab.add(word)\nprint('уникальных токенов, втретившихся больше 2 раз:', len(filtered_vocab))","metadata":{"id":"5uMSYgKN986p","execution":{"iopub.status.busy":"2021-12-03T16:18:38.308846Z","iopub.execute_input":"2021-12-03T16:18:38.309128Z","iopub.status.idle":"2021-12-03T16:18:38.410206Z","shell.execute_reply.started":"2021-12-03T16:18:38.309099Z","shell.execute_reply":"2021-12-03T16:18:38.409014Z"},"trusted":true},"execution_count":354,"outputs":[]},{"cell_type":"code","source":"word2id = {'PAD':0}\n\nfor word in filtered_vocab:\n    word2id[word] = len(word2id)\n\nid2word = {i:word for word, i in word2id.items()}","metadata":{"id":"suOD4Rha9a2M","execution":{"iopub.status.busy":"2021-12-03T16:18:38.750727Z","iopub.execute_input":"2021-12-03T16:18:38.751016Z","iopub.status.idle":"2021-12-03T16:18:38.778527Z","shell.execute_reply.started":"2021-12-03T16:18:38.750987Z","shell.execute_reply":"2021-12-03T16:18:38.777546Z"},"trusted":true},"execution_count":355,"outputs":[]},{"cell_type":"code","source":"symbol_vocab = Counter()\nfor text in all_tweets_data['text']:\n    symbol_vocab.update(list(text))\nprint('всего уникальных символов:', len(symbol_vocab))","metadata":{"id":"TNdh5exN9a2N","execution":{"iopub.status.busy":"2021-12-03T16:18:39.200753Z","iopub.execute_input":"2021-12-03T16:18:39.201020Z","iopub.status.idle":"2021-12-03T16:18:40.859871Z","shell.execute_reply.started":"2021-12-03T16:18:39.200968Z","shell.execute_reply":"2021-12-03T16:18:40.858772Z"},"trusted":true},"execution_count":356,"outputs":[]},{"cell_type":"code","source":"symbol2id = {'PAD':0}\n\nfor symbol in symbol_vocab:\n    symbol2id[symbol] = len(symbol2id)\n\nid2symbol = {i:symbol for symbol, i in symbol2id.items()}","metadata":{"id":"VK25ReL_9a2N","execution":{"iopub.status.busy":"2021-12-03T16:18:40.862083Z","iopub.execute_input":"2021-12-03T16:18:40.863128Z","iopub.status.idle":"2021-12-03T16:18:40.869572Z","shell.execute_reply.started":"2021-12-03T16:18:40.863082Z","shell.execute_reply":"2021-12-03T16:18:40.868482Z"},"trusted":true},"execution_count":357,"outputs":[]},{"cell_type":"code","source":"class TweetsDatasetWordSymbolRaw(Dataset):\n\n    def __init__(self, dataset, word2id, symbol2id, DEVICE):\n        self.dataset = dataset['text'].values\n        self.word2id = word2id\n        self.symbol2id = symbol2id\n\n        self.length = dataset.shape[0]\n        self.target = torch.Tensor(dataset['tone'].values)\n        self.device = DEVICE\n\n    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n        return self.length\n\n    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n        symbols = list(self.dataset[index])\n        symbol_ids = torch.LongTensor([self.symbol2id[symbol] for symbol in symbols if symbol in self.symbol2id])\n        tokens = self.dataset[index].split()\n        word_ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n        y = [self.target[index]]\n        return word_ids, symbol_ids, y\n\n    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n    # он понадобится для DataLoader во время итерации по батчам\n      word_ids, symbol_ids, y = list(zip(*batch))\n      padded_words = pad_sequence(word_ids, batch_first=True).to(self.device)\n      padded_symbols = pad_sequence(symbol_ids, batch_first=True).to(self.device)\n      #мы хотим применять BCELoss, он будет брать на вход predicted размера batch_size x 1 (так как для каждого семпла модель будет отдавать одно число), target размера batch_size x 1 \n      y = torch.Tensor(y).to(self.device) # tuple ([1], [0], [1])  -> Tensor [[1.], [0.], [1.]]\n      return padded_words, padded_symbols, y","metadata":{"id":"PX6uGL1D9a2N","execution":{"iopub.status.busy":"2021-12-03T16:18:40.871390Z","iopub.execute_input":"2021-12-03T16:18:40.871945Z","iopub.status.idle":"2021-12-03T16:18:40.891163Z","shell.execute_reply.started":"2021-12-03T16:18:40.871902Z","shell.execute_reply":"2021-12-03T16:18:40.890273Z"},"trusted":true},"execution_count":358,"outputs":[]},{"cell_type":"code","source":"train_dataset = TweetsDatasetWordSymbolRaw(train_sentences, word2id, symbol2id, DEVICE)\ntrain_sampler = RandomSampler(train_dataset)\ntrain_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)\nbatch = next(iter(train_iterator))\nbatch[0].shape","metadata":{"id":"FBGWn9uo9a2N","execution":{"iopub.status.busy":"2021-12-03T16:18:41.224680Z","iopub.execute_input":"2021-12-03T16:18:41.225284Z","iopub.status.idle":"2021-12-03T16:18:41.313726Z","shell.execute_reply.started":"2021-12-03T16:18:41.225212Z","shell.execute_reply":"2021-12-03T16:18:41.312658Z"},"trusted":true},"execution_count":359,"outputs":[]},{"cell_type":"code","source":"val_dataset = TweetsDatasetWordSymbolRaw(val_sentences, word2id, symbol2id, DEVICE)\nval_sampler = SequentialSampler(val_dataset)\nval_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)","metadata":{"id":"ZgGL4uy49a2N","execution":{"iopub.status.busy":"2021-12-03T16:18:41.554691Z","iopub.execute_input":"2021-12-03T16:18:41.554990Z","iopub.status.idle":"2021-12-03T16:18:41.562261Z","shell.execute_reply.started":"2021-12-03T16:18:41.554960Z","shell.execute_reply":"2021-12-03T16:18:41.561091Z"},"trusted":true},"execution_count":360,"outputs":[]},{"cell_type":"code","source":"model = CNNWordSymbol(len(word2id), len(symbol2id), 2)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.BCELoss()  \n\n# веса модели и значения лосса храним там же, где и все остальные тензоры\nmodel = model.to(DEVICE)\ncriterion = criterion.to(DEVICE)","metadata":{"id":"z1uhmQQV-IOx","execution":{"iopub.status.busy":"2021-12-03T16:18:42.181692Z","iopub.execute_input":"2021-12-03T16:18:42.182365Z","iopub.status.idle":"2021-12-03T16:18:42.232574Z","shell.execute_reply.started":"2021-12-03T16:18:42.182331Z","shell.execute_reply":"2021-12-03T16:18:42.231681Z"},"trusted":true},"execution_count":361,"outputs":[]},{"cell_type":"code","source":"losses = []\nlosses_eval = []\nf1s = []\nf1s_eval = []\n\nfor i in range(10):\n    print(f'\\nstarting Epoch {i}')\n    print('Training...')\n    epoch_loss = train(model, train_iterator, optimizer, criterion)\n    losses.append(epoch_loss)\n    print('\\nEvaluating on train...')\n    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n    f1s.append(f1_on_train)\n    print('\\nEvaluating on test...')\n    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n    losses_eval.append(epoch_loss_on_test)\n    f1s_eval.append(f1_on_test)","metadata":{"id":"qH523oAl-NM7","execution":{"iopub.status.busy":"2021-12-03T16:18:42.655395Z","iopub.execute_input":"2021-12-03T16:18:42.655923Z","iopub.status.idle":"2021-12-03T16:21:00.222907Z","shell.execute_reply.started":"2021-12-03T16:18:42.655890Z","shell.execute_reply":"2021-12-03T16:21:00.221943Z"},"trusted":true},"execution_count":362,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 6))\nplt.title('Train')\nplt.xlabel('Iterations')\nplt.ylabel('Losses')\nplt.grid()\nax.plot(losses, label='Trainig loss')\nax.plot(losses_eval, label='Evaluation loss')\nax.legend()\nplt.show()","metadata":{"id":"Q5AXCLdS-hPf","execution":{"iopub.status.busy":"2021-12-03T16:21:00.225237Z","iopub.execute_input":"2021-12-03T16:21:00.225511Z","iopub.status.idle":"2021-12-03T16:21:00.487306Z","shell.execute_reply.started":"2021-12-03T16:21:00.225456Z","shell.execute_reply":"2021-12-03T16:21:00.486162Z"},"trusted":true},"execution_count":363,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 6))\nplt.title('Train')\nplt.xlabel('Iterations')\nplt.ylabel('Losses')\nplt.grid()\nax.plot(f1s, label='Train F1')\nax.plot(f1s_eval, label='Evaluation F1')\nax.legend()\nplt.show()","metadata":{"id":"lZWKo-3p-hPf","execution":{"iopub.status.busy":"2021-12-03T16:21:00.488834Z","iopub.execute_input":"2021-12-03T16:21:00.489472Z","iopub.status.idle":"2021-12-03T16:21:00.761763Z","shell.execute_reply.started":"2021-12-03T16:21:00.489423Z","shell.execute_reply":"2021-12-03T16:21:00.760498Z"},"trusted":true},"execution_count":364,"outputs":[]},{"cell_type":"code","source":"print('F1 training score: ', f1s[-1])\nprint('F1 evaluation score: ', f1s_eval[-1])\nprint('Training loss: ',losses[-1])\nprint('Evaluation loss: ',losses_eval[-1])","metadata":{"id":"LUqKcLRB-hPf","execution":{"iopub.status.busy":"2021-12-03T16:21:00.764483Z","iopub.execute_input":"2021-12-03T16:21:00.765070Z","iopub.status.idle":"2021-12-03T16:21:00.779150Z","shell.execute_reply.started":"2021-12-03T16:21:00.765007Z","shell.execute_reply":"2021-12-03T16:21:00.777971Z"},"trusted":true},"execution_count":365,"outputs":[]},{"cell_type":"code","source":"val_sentences['predicted']  = predict_word_symbol(model, val_iterator)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T16:21:00.780811Z","iopub.execute_input":"2021-12-03T16:21:00.781625Z","iopub.status.idle":"2021-12-03T16:21:01.930343Z","shell.execute_reply.started":"2021-12-03T16:21:00.781581Z","shell.execute_reply":"2021-12-03T16:21:01.929148Z"},"trusted":true},"execution_count":366,"outputs":[]},{"cell_type":"code","source":"# TP\nval_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 1)][['text']]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T16:21:56.584131Z","iopub.execute_input":"2021-12-03T16:21:56.584990Z","iopub.status.idle":"2021-12-03T16:21:56.643271Z","shell.execute_reply.started":"2021-12-03T16:21:56.584909Z","shell.execute_reply":"2021-12-03T16:21:56.642173Z"},"trusted":true},"execution_count":371,"outputs":[]},{"cell_type":"code","source":"# FN\nval_sentences[(val_sentences['tone'] == 1) & (val_sentences['predicted'] == 0)][['text']]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T16:21:57.424982Z","iopub.execute_input":"2021-12-03T16:21:57.425287Z","iopub.status.idle":"2021-12-03T16:21:57.464103Z","shell.execute_reply.started":"2021-12-03T16:21:57.425246Z","shell.execute_reply":"2021-12-03T16:21:57.463054Z"},"trusted":true},"execution_count":372,"outputs":[]},{"cell_type":"code","source":"# FP\nval_sentences[(val_sentences['tone'] == 0) & (val_sentences['predicted'] == 1)][['text']]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T16:21:57.955178Z","iopub.execute_input":"2021-12-03T16:21:57.955702Z","iopub.status.idle":"2021-12-03T16:21:58.013650Z","shell.execute_reply.started":"2021-12-03T16:21:57.955657Z","shell.execute_reply":"2021-12-03T16:21:58.012540Z"},"trusted":true},"execution_count":373,"outputs":[]},{"cell_type":"markdown","source":"Стало намного круче, предобработка плохо влияет на качество модели. А происходит это потому, что для классификации используются специальные символы, известные как эмодзи или смайлики. По ним модели проще определять тональность, например:\n\n@KristinaEchelon тоже верно хд\\nОни еще и коричневые. Дааа, пускай будут соплями :DDD - **POS** изза :DDD и так далее.\n\nЭто также можно проследить в предыдущих моделях, часто смайлик XD пишут как хд, это помогает классифицировать предложение как позитивное","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}