{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"# ü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§óü§ó","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:19:59.817894Z","iopub.execute_input":"2022-03-27T18:19:59.818619Z","iopub.status.idle":"2022-03-27T18:19:59.840803Z","shell.execute_reply.started":"2022-03-27T18:19:59.818531Z","shell.execute_reply":"2022-03-27T18:19:59.840140Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:19:59.842608Z","iopub.execute_input":"2022-03-27T18:19:59.843118Z","iopub.status.idle":"2022-03-27T18:20:08.487352Z","shell.execute_reply.started":"2022-03-27T18:19:59.843080Z","shell.execute_reply":"2022-03-27T18:20:08.486534Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.16.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.3)\nRequirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.11.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.49)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install datasets","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:20:08.488883Z","iopub.execute_input":"2022-03-27T18:20:08.489146Z","iopub.status.idle":"2022-03-27T18:20:16.407641Z","shell.execute_reply.started":"2022-03-27T18:20:08.489110Z","shell.execute_reply":"2022-03-27T18:20:16.406811Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (1.18.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.4)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.62.3)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.4.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.20.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.12.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.11.3)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.26.0)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (5.2.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.2.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\nfrom transformers import BertModel, AutoTokenizer, BertTokenizer, PreTrainedTokenizerFast, AdamW, get_linear_schedule_with_warmup, AutoModelForSequenceClassification\nimport torch.nn.functional as F\nimport torch\nfrom torch.nn import CrossEntropyLoss\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom torch import nn, optim\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.utils.data import Dataset, DataLoader\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nHF_HUB_MODEL = 'prajjwal1/bert-medium'","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:20:16.410266Z","iopub.execute_input":"2022-03-27T18:20:16.410539Z","iopub.status.idle":"2022-03-27T18:20:22.778290Z","shell.execute_reply.started":"2022-03-27T18:20:16.410503Z","shell.execute_reply":"2022-03-27T18:20:22.777511Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 0. Dataset Preparation","metadata":{}},{"cell_type":"markdown","source":"–°–¥–µ–ª–∞–µ–º —à–∞—Ñ—Ñ–ª –¥–∞—Ç–∞—Å–µ—Ç–∞, —á—Ç–æ–±—ã —É –Ω–∞—Å –±—ã–ª–∏ –ø—Ä–∏–º–µ—Ä—ã —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ (–≤—Ä–æ–¥–µ –∫–∞–∫ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –æ–Ω–∏ –∏–¥—É—Ç –ø–æ –ø–æ—Ä—è–∫—É). –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π —É–∫–∞–∂–µ–º —Å–∏–¥, –Ω–æ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–Ω–∏ –∏ —Ç–µ –∂–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nimdb_dataset = load_dataset('imdb')\n\nsmall_train_dataset = imdb_dataset[\"train\"].shuffle(seed=42).select(range(5000))\nsmall_test_dataset = imdb_dataset[\"test\"].shuffle(seed=42).select(range(1000))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:20:22.781118Z","iopub.execute_input":"2022-03-27T18:20:22.781375Z","iopub.status.idle":"2022-03-27T18:21:04.955297Z","shell.execute_reply.started":"2022-03-27T18:20:22.781342Z","shell.execute_reply":"2022-03-27T18:21:04.954553Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b676bc10b2294468bf36d07768e31545"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa1159b1fae743bb914713781d71c5a9"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/84.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af683a96516e41f0a4150662d9b68f59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf2d8e3dcbb7472f96b6782e2dbfd9bc"}},"metadata":{}}]},{"cell_type":"markdown","source":"–†–∞–∑–¥–µ–ª–∏–º –¥–∞—Ç–∞—Å–µ—Ç","metadata":{}},{"cell_type":"code","source":"train_texts = small_train_dataset[\"text\"]\ntrain_labels = small_train_dataset[\"label\"]\ntest_texts = small_test_dataset[\"text\"][:1000]\ntest_labels = small_test_dataset[\"label\"][:1000]\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:04.958322Z","iopub.execute_input":"2022-03-27T18:21:04.958557Z","iopub.status.idle":"2022-03-27T18:21:05.056118Z","shell.execute_reply.started":"2022-03-27T18:21:04.958530Z","shell.execute_reply":"2022-03-27T18:21:05.055280Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(HF_HUB_MODEL)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:05.057512Z","iopub.execute_input":"2022-03-27T18:21:05.057985Z","iopub.status.idle":"2022-03-27T18:21:11.098279Z","shell.execute_reply.started":"2022-03-27T18:21:05.057935Z","shell.execute_reply":"2022-03-27T18:21:11.097259Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/286 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d86386b07d445609ed198ac38cc3d6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e3cfc47eb594093bb6c45728ad63865"}},"metadata":{}}]},{"cell_type":"markdown","source":"–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏—Å–ø–æ–ª—å–∑—É—è padding –∏ truncation, —Ç–∞–∫ –∂–µ —É–∫–∞–∂–µ–º max_len –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ù–∞—à–∞ –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ –¥–∏–Ω—ã 512","metadata":{}},{"cell_type":"code","source":"train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_token_type_ids=False, return_attention_mask=True, max_length=512)\nval_encodings = tokenizer(val_texts,truncation=True, padding=True, return_token_type_ids=False, return_attention_mask=True, max_length=512)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, return_token_type_ids=False, return_attention_mask=True, max_length=512)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:11.099533Z","iopub.execute_input":"2022-03-27T18:21:11.099806Z","iopub.status.idle":"2022-03-27T18:21:15.406195Z","shell.execute_reply.started":"2022-03-27T18:21:11.099768Z","shell.execute_reply":"2022-03-27T18:21:15.405430Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class IMDbDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = IMDbDataset(train_encodings, train_labels)\nval_dataset = IMDbDataset(val_encodings, val_labels)\ntest_dataset = IMDbDataset(test_encodings, test_labels)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:15.409157Z","iopub.execute_input":"2022-03-27T18:21:15.409429Z","iopub.status.idle":"2022-03-27T18:21:15.418381Z","shell.execute_reply.started":"2022-03-27T18:21:15.409394Z","shell.execute_reply":"2022-03-27T18:21:15.417655Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Metric","metadata":{}},{"cell_type":"markdown","source":"–ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å binary f-score —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:15.419675Z","iopub.execute_input":"2022-03-27T18:21:15.419935Z","iopub.status.idle":"2022-03-27T18:21:15.551051Z","shell.execute_reply.started":"2022-03-27T18:21:15.419898Z","shell.execute_reply":"2022-03-27T18:21:15.550202Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:15.552631Z","iopub.execute_input":"2022-03-27T18:21:15.553218Z","iopub.status.idle":"2022-03-27T18:21:15.560035Z","shell.execute_reply.started":"2022-03-27T18:21:15.553177Z","shell.execute_reply":"2022-03-27T18:21:15.559326Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Training arguments","metadata":{}},{"cell_type":"markdown","source":"–î–æ–±–∞–≤–∏–º evaluation_strategy = steps, –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤ –º—ã –±—É–¥–µ–º –¥–µ–ª–∞—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é. –¢—É—Ç —Ç–∞–∫–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –≤—Å–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π","metadata":{}},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=2,              # total number of training epochs\n    per_device_train_batch_size=8,  # batch size per device during training\n    per_device_eval_batch_size=16,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=100,\n    report_to=None,\n    evaluation_strategy = 'steps'\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:15.561427Z","iopub.execute_input":"2022-03-27T18:21:15.561804Z","iopub.status.idle":"2022-03-27T18:21:15.960356Z","shell.execute_reply.started":"2022-03-27T18:21:15.561767Z","shell.execute_reply":"2022-03-27T18:21:15.959669Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 1. Basic SentimentClassifier ","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"–ù–µ–º–Ω–æ–≥–æ –∏–∑–º–µ–Ω–∏–º –∫–ª–∞—Å—Å SentimentClassifier —á—Ç–æ–±—ã –∑–∞—Å—É–Ω—É—Ç—å –µ–≥–æ –≤ Trainer (—Ä–∞–Ω—å—à–µ –æ–Ω –Ω–µ —É–º–µ–ª —Ä–∞–±–æ—Ç–∞—Ç—å —Å labels –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç—Å—è –¥–ª—è Trainer)","metadata":{}},{"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(HF_HUB_MODEL)\n        self.n_classes = n_classes\n        self.dropout = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n    def forward( # —Å–æ—Ä—Å –∫–æ–¥ –∏–∑ BertModel\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=False,\n    ):\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            return_dict=False)\n        \n        pooled_output = outputs[1]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.out(pooled_output)\n        \n        loss = None # —Å–æ—Ä—Å –∫–æ–¥ –∏–∑ BertModel, –Ω—É–∂–Ω–æ –±—ã–ª–æ —Å–¥–µ–ª–∞—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–π –ª–æ—Å—Å\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.n_classes), labels.view(-1))\n        output = (logits,)\n        return ((loss,) + output) if loss is not None else output","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:15.961722Z","iopub.execute_input":"2022-03-27T18:21:15.961986Z","iopub.status.idle":"2022-03-27T18:21:15.972992Z","shell.execute_reply.started":"2022-03-27T18:21:15.961938Z","shell.execute_reply":"2022-03-27T18:21:15.971166Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"model1 = SentimentClassifier(2) \nmodel1 = model1.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:15.974085Z","iopub.execute_input":"2022-03-27T18:21:15.974260Z","iopub.status.idle":"2022-03-27T18:21:28.242847Z","shell.execute_reply.started":"2022-03-27T18:21:15.974238Z","shell.execute_reply":"2022-03-27T18:21:28.242098Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/159M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c10ef0037e6428ebfc0657029125767"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model1,                         # the instantiated ü§ó Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset,            # evaluation dataset\n    compute_metrics = compute_metrics    # metrics to evaluate\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:28.244132Z","iopub.execute_input":"2022-03-27T18:21:28.244398Z","iopub.status.idle":"2022-03-27T18:25:46.408386Z","shell.execute_reply.started":"2022-03-27T18:21:28.244364Z","shell.execute_reply":"2022-03-27T18:25:46.407502Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 4000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 04:17, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.693700</td>\n      <td>0.617841</td>\n      <td>0.681000</td>\n      <td>0.728511</td>\n      <td>0.639761</td>\n      <td>0.845850</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.511600</td>\n      <td>0.472773</td>\n      <td>0.785000</td>\n      <td>0.813206</td>\n      <td>0.725581</td>\n      <td>0.924901</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.405700</td>\n      <td>0.416051</td>\n      <td>0.845000</td>\n      <td>0.833154</td>\n      <td>0.914894</td>\n      <td>0.764822</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.417800</td>\n      <td>0.418789</td>\n      <td>0.841000</td>\n      <td>0.845781</td>\n      <td>0.830476</td>\n      <td>0.861660</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.391600</td>\n      <td>0.365908</td>\n      <td>0.840000</td>\n      <td>0.828326</td>\n      <td>0.906103</td>\n      <td>0.762846</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.355200</td>\n      <td>0.616473</td>\n      <td>0.799000</td>\n      <td>0.829517</td>\n      <td>0.726597</td>\n      <td>0.966403</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.311600</td>\n      <td>0.409561</td>\n      <td>0.874000</td>\n      <td>0.876953</td>\n      <td>0.866795</td>\n      <td>0.887352</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.268700</td>\n      <td>0.329048</td>\n      <td>0.882000</td>\n      <td>0.884314</td>\n      <td>0.877432</td>\n      <td>0.891304</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.250600</td>\n      <td>0.359364</td>\n      <td>0.886000</td>\n      <td>0.887795</td>\n      <td>0.884314</td>\n      <td>0.891304</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.270900</td>\n      <td>0.371197</td>\n      <td>0.886000</td>\n      <td>0.888672</td>\n      <td>0.878378</td>\n      <td>0.899209</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-500\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1000\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.38774579620361327, metrics={'train_runtime': 258.0585, 'train_samples_per_second': 31.001, 'train_steps_per_second': 3.875, 'total_flos': 0.0, 'train_loss': 0.38774579620361327, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:25:46.409868Z","iopub.execute_input":"2022-03-27T18:25:46.410246Z","iopub.status.idle":"2022-03-27T18:25:53.292727Z","shell.execute_reply.started":"2022-03-27T18:25:46.410208Z","shell.execute_reply":"2022-03-27T18:25:53.292085Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:06]\n    </div>\n    "},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.3712855279445648,\n 'test_accuracy': 0.89,\n 'test_f1': 0.8877551020408164,\n 'test_precision': 0.8841463414634146,\n 'test_recall': 0.8913934426229508,\n 'test_runtime': 6.8724,\n 'test_samples_per_second': 145.509,\n 'test_steps_per_second': 9.167,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"# 2. SentimentClassifier with CLS token","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"–í –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∫–ª–∞—Å—Å –¥–æ–±–∞–≤–∏–º CLS —Ç–æ–∫–µ–Ω. CLS —Ç–æ–∫–µ–Ω —ç—Ç–æ –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –∏–∑ last_hidden_state","metadata":{}},{"cell_type":"code","source":"class CLSSentimentClassifier(nn.Module):\n    \n    def __init__(self, n_classes):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(HF_HUB_MODEL)\n        self.n_classes = n_classes\n        self.dropout = nn.Dropout(p=0.3)\n        self.linear = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n        self.out = nn.Linear(self.bert.config.hidden_size + 512, n_classes) # —Ç–∞–∫ –∫–∞–∫ –¥–æ–±–∞–≤–ª—è–µ–º CLS —Ç–æ–∫–µ–Ω, –Ω–∞–¥–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å–∏—Ç—å –≤—ã—Ö–æ–¥–∞ –Ω–∞ hidden_size\n        \n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=False,\n    ):\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            return_dict=False)\n        \n\n        CLS_token = outputs[0][:,0,:] # CLS —ç—Ç–æ –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω last_hidden_state\n        pooled_output = self.linear(self.dropout(outputs[1])) \n        stacked = torch.hstack([CLS_token, pooled_output]) # —Å–∫–ª–µ–∏–º CLS token –∏ pooled output\n\n        logits = self.out(stacked)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.n_classes), labels.view(-1))\n        output = (logits,)\n        return ((loss,) + output) if loss is not None else output","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:25:53.294207Z","iopub.execute_input":"2022-03-27T18:25:53.294484Z","iopub.status.idle":"2022-03-27T18:25:53.304537Z","shell.execute_reply.started":"2022-03-27T18:25:53.294448Z","shell.execute_reply":"2022-03-27T18:25:53.303812Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"model2 = CLSSentimentClassifier(2) \nmodel2 = model2.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:25:53.305927Z","iopub.execute_input":"2022-03-27T18:25:53.306428Z","iopub.status.idle":"2022-03-27T18:25:54.847099Z","shell.execute_reply.started":"2022-03-27T18:25:53.306389Z","shell.execute_reply":"2022-03-27T18:25:54.846386Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/prajjwal1/bert-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/288b0ee1e79a7c3fe770ab8a84ece013c573e7d226ccb5d9ffad317b3419faac.4344f82f77799c092b30b2e0d3749c809f82df14c5993e43dbbdc52f5a0d86e0\nModel config BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 512,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2048,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 8,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/prajjwal1/bert-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/dabb6f3bc29449f038f41cb09eb1a693eee2bee3dab8afff878a2910fa73a171.b722b1c13187b9ed20e5e36ab761041218e88d502895424e3ed2516bc9693089\nSome weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of BertModel were initialized from the model checkpoint at prajjwal1/bert-medium.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model2,                         # the instantiated ü§ó Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset,            # evaluation dataset\n    compute_metrics = compute_metrics    # metrics to evaluate\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:25:54.848406Z","iopub.execute_input":"2022-03-27T18:25:54.848807Z","iopub.status.idle":"2022-03-27T18:30:14.677918Z","shell.execute_reply.started":"2022-03-27T18:25:54.848682Z","shell.execute_reply":"2022-03-27T18:30:14.677165Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 4000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 04:19, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.685500</td>\n      <td>0.585503</td>\n      <td>0.725000</td>\n      <td>0.744186</td>\n      <td>0.702988</td>\n      <td>0.790514</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.485100</td>\n      <td>0.441349</td>\n      <td>0.824000</td>\n      <td>0.839416</td>\n      <td>0.779661</td>\n      <td>0.909091</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.410100</td>\n      <td>0.431481</td>\n      <td>0.835000</td>\n      <td>0.844193</td>\n      <td>0.808318</td>\n      <td>0.883399</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.443000</td>\n      <td>0.433221</td>\n      <td>0.851000</td>\n      <td>0.859301</td>\n      <td>0.822785</td>\n      <td>0.899209</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.403900</td>\n      <td>0.327260</td>\n      <td>0.858000</td>\n      <td>0.859684</td>\n      <td>0.859684</td>\n      <td>0.859684</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.314100</td>\n      <td>0.424729</td>\n      <td>0.851000</td>\n      <td>0.860878</td>\n      <td>0.815929</td>\n      <td>0.911067</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.342900</td>\n      <td>0.387718</td>\n      <td>0.867000</td>\n      <td>0.865521</td>\n      <td>0.886128</td>\n      <td>0.845850</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.256000</td>\n      <td>0.334733</td>\n      <td>0.877000</td>\n      <td>0.873065</td>\n      <td>0.913607</td>\n      <td>0.835968</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.255200</td>\n      <td>0.405889</td>\n      <td>0.885000</td>\n      <td>0.888889</td>\n      <td>0.869565</td>\n      <td>0.909091</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.263200</td>\n      <td>0.395804</td>\n      <td>0.888000</td>\n      <td>0.888668</td>\n      <td>0.894000</td>\n      <td>0.883399</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-500\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1000\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.38589393615722656, metrics={'train_runtime': 259.7948, 'train_samples_per_second': 30.794, 'train_steps_per_second': 3.849, 'total_flos': 0.0, 'train_loss': 0.38589393615722656, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:30:14.679539Z","iopub.execute_input":"2022-03-27T18:30:14.684156Z","iopub.status.idle":"2022-03-27T18:30:21.594479Z","shell.execute_reply.started":"2022-03-27T18:30:14.684112Z","shell.execute_reply":"2022-03-27T18:30:21.593810Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:06]\n    </div>\n    "},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.3970975875854492,\n 'test_accuracy': 0.882,\n 'test_f1': 0.8773388773388774,\n 'test_precision': 0.890295358649789,\n 'test_recall': 0.8647540983606558,\n 'test_runtime': 6.9012,\n 'test_samples_per_second': 144.902,\n 'test_steps_per_second': 9.129,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"# 3. Transformers - BertForSequenceClassification","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"–¢—É—Ç –Ω–∏—á–µ–≥–æ —Å–ª–æ–∂–Ω–æ–≥–æ: –≤–æ–∑—å–º–µ–º BertForSequenceClassification –∏ –∑–∞–≥—Ä—É–∑–∏–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å Huggingface","metadata":{}},{"cell_type":"code","source":"from transformers import  BertForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:30:21.595805Z","iopub.execute_input":"2022-03-27T18:30:21.596094Z","iopub.status.idle":"2022-03-27T18:30:21.601417Z","shell.execute_reply.started":"2022-03-27T18:30:21.596058Z","shell.execute_reply":"2022-03-27T18:30:21.600645Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"model3 = BertForSequenceClassification.from_pretrained(HF_HUB_MODEL)\nmodel3 = model3.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:30:21.603513Z","iopub.execute_input":"2022-03-27T18:30:21.604218Z","iopub.status.idle":"2022-03-27T18:30:23.092874Z","shell.execute_reply.started":"2022-03-27T18:30:21.604181Z","shell.execute_reply":"2022-03-27T18:30:23.092123Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/prajjwal1/bert-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/288b0ee1e79a7c3fe770ab8a84ece013c573e7d226ccb5d9ffad317b3419faac.4344f82f77799c092b30b2e0d3749c809f82df14c5993e43dbbdc52f5a0d86e0\nModel config BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 512,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2048,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 8,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/prajjwal1/bert-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/dabb6f3bc29449f038f41cb09eb1a693eee2bee3dab8afff878a2910fa73a171.b722b1c13187b9ed20e5e36ab761041218e88d502895424e3ed2516bc9693089\nSome weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model3,                         # the instantiated ü§ó Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset,            # evaluation dataset\n    compute_metrics = compute_metrics    # metrics to evaluate\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:30:23.094302Z","iopub.execute_input":"2022-03-27T18:30:23.094570Z","iopub.status.idle":"2022-03-27T18:34:41.744428Z","shell.execute_reply.started":"2022-03-27T18:30:23.094535Z","shell.execute_reply":"2022-03-27T18:34:41.742773Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 4000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 04:18, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.679300</td>\n      <td>0.618526</td>\n      <td>0.735000</td>\n      <td>0.749764</td>\n      <td>0.717902</td>\n      <td>0.784585</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.497700</td>\n      <td>0.349631</td>\n      <td>0.861000</td>\n      <td>0.862784</td>\n      <td>0.861933</td>\n      <td>0.863636</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.394300</td>\n      <td>0.403675</td>\n      <td>0.840000</td>\n      <td>0.851577</td>\n      <td>0.802448</td>\n      <td>0.907115</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.379700</td>\n      <td>0.389311</td>\n      <td>0.850000</td>\n      <td>0.855769</td>\n      <td>0.833333</td>\n      <td>0.879447</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.392800</td>\n      <td>0.374978</td>\n      <td>0.844000</td>\n      <td>0.832976</td>\n      <td>0.908879</td>\n      <td>0.768775</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.348000</td>\n      <td>0.434627</td>\n      <td>0.857000</td>\n      <td>0.871287</td>\n      <td>0.800000</td>\n      <td>0.956522</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.327500</td>\n      <td>0.421971</td>\n      <td>0.855000</td>\n      <td>0.844920</td>\n      <td>0.920746</td>\n      <td>0.780632</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.292900</td>\n      <td>0.295128</td>\n      <td>0.891000</td>\n      <td>0.893451</td>\n      <td>0.883946</td>\n      <td>0.903162</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.229800</td>\n      <td>0.322991</td>\n      <td>0.890000</td>\n      <td>0.894636</td>\n      <td>0.868030</td>\n      <td>0.922925</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.262500</td>\n      <td>0.324668</td>\n      <td>0.895000</td>\n      <td>0.897361</td>\n      <td>0.887814</td>\n      <td>0.907115</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-500\nConfiguration saved in ./results/checkpoint-500/config.json\nModel weights saved in ./results/checkpoint-500/pytorch_model.bin\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1000\nConfiguration saved in ./results/checkpoint-1000/config.json\nModel weights saved in ./results/checkpoint-1000/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.3804571762084961, metrics={'train_runtime': 258.609, 'train_samples_per_second': 30.935, 'train_steps_per_second': 3.867, 'total_flos': 626289328128000.0, 'train_loss': 0.3804571762084961, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:34:41.750808Z","iopub.execute_input":"2022-03-27T18:34:41.753069Z","iopub.status.idle":"2022-03-27T18:34:48.606617Z","shell.execute_reply.started":"2022-03-27T18:34:41.753030Z","shell.execute_reply":"2022-03-27T18:34:48.605920Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:06]\n    </div>\n    "},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.38930660486221313,\n 'test_accuracy': 0.878,\n 'test_f1': 0.8747433264887063,\n 'test_precision': 0.8765432098765432,\n 'test_recall': 0.8729508196721312,\n 'test_runtime': 6.8438,\n 'test_samples_per_second': 146.118,\n 'test_steps_per_second': 9.205,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"# 4. SentimentClassifier with aggregated CLS-tokens from layers","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"–î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –∞–≥–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å CLS-—Ç–æ–∫–µ–Ω—ã –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–µ–≤, —Å–¥–µ–ª–∞–µ–º mean pooling –ø–æ —Å–ª–æ—é hidden_state","metadata":{}},{"cell_type":"code","source":"class AGGCLSSentimentClassifier(nn.Module):\n    \n    def __init__(self, n_classes):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(HF_HUB_MODEL)\n        self.n_classes = n_classes\n        self.dropout = nn.Dropout(p=0.3)\n        self.linear = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n        self.out = nn.Linear(self.bert.config.hidden_size + 512, n_classes) \n        \n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=False,\n    ):\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            output_hidden_states=True,\n                            return_dict=False)\n        \n        output_hidden_states = outputs[2]\n        \n        hidden_states = torch.stack(output_hidden_states) # –±–µ—Ä–µ–º —Å–ª–æ–∏ hidden_state —Å–ª–æ–∏\n        \n        CLS_tokens = torch.mean(hidden_states[:, :, 0], 0) # –¥–µ–ª–∞–µ–º mean pooling\n        pooled_output = self.linear(self.dropout(outputs[1]))\n        stacked = torch.hstack([CLS_tokens, pooled_output]) # —Å–æ–µ–¥–∏–Ω—è–µ–º mean pooling –∏ pooled output\n    \n\n        logits = self.out(stacked)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.n_classes), labels.view(-1))\n        output = (logits,)\n        return ((loss,) + output) if loss is not None else output","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:34:48.607713Z","iopub.execute_input":"2022-03-27T18:34:48.607962Z","iopub.status.idle":"2022-03-27T18:34:48.618921Z","shell.execute_reply.started":"2022-03-27T18:34:48.607926Z","shell.execute_reply":"2022-03-27T18:34:48.618277Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"model4 = AGGCLSSentimentClassifier(2) \nmodel4 = model4.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:34:48.620235Z","iopub.execute_input":"2022-03-27T18:34:48.620660Z","iopub.status.idle":"2022-03-27T18:34:50.092448Z","shell.execute_reply.started":"2022-03-27T18:34:48.620623Z","shell.execute_reply":"2022-03-27T18:34:50.091681Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/prajjwal1/bert-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/288b0ee1e79a7c3fe770ab8a84ece013c573e7d226ccb5d9ffad317b3419faac.4344f82f77799c092b30b2e0d3749c809f82df14c5993e43dbbdc52f5a0d86e0\nModel config BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 512,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2048,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 8,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/prajjwal1/bert-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/dabb6f3bc29449f038f41cb09eb1a693eee2bee3dab8afff878a2910fa73a171.b722b1c13187b9ed20e5e36ab761041218e88d502895424e3ed2516bc9693089\nSome weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of BertModel were initialized from the model checkpoint at prajjwal1/bert-medium.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model4,                         # the instantiated ü§ó Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset,            # evaluation dataset\n    compute_metrics = compute_metrics    # metrics to evaluate\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:34:50.093626Z","iopub.execute_input":"2022-03-27T18:34:50.094421Z","iopub.status.idle":"2022-03-27T18:39:10.165241Z","shell.execute_reply.started":"2022-03-27T18:34:50.094378Z","shell.execute_reply":"2022-03-27T18:39:10.164483Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 4000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 04:19, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.707200</td>\n      <td>0.647586</td>\n      <td>0.679000</td>\n      <td>0.658147</td>\n      <td>0.713626</td>\n      <td>0.610672</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.532500</td>\n      <td>0.388303</td>\n      <td>0.839000</td>\n      <td>0.842002</td>\n      <td>0.836257</td>\n      <td>0.847826</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.400700</td>\n      <td>0.432419</td>\n      <td>0.849000</td>\n      <td>0.850643</td>\n      <td>0.851485</td>\n      <td>0.849802</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.430600</td>\n      <td>0.525616</td>\n      <td>0.831000</td>\n      <td>0.848158</td>\n      <td>0.777595</td>\n      <td>0.932806</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.400000</td>\n      <td>0.347810</td>\n      <td>0.861000</td>\n      <td>0.857436</td>\n      <td>0.891258</td>\n      <td>0.826087</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.361200</td>\n      <td>0.434164</td>\n      <td>0.847000</td>\n      <td>0.863026</td>\n      <td>0.788871</td>\n      <td>0.952569</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.311800</td>\n      <td>0.376082</td>\n      <td>0.881000</td>\n      <td>0.875393</td>\n      <td>0.930958</td>\n      <td>0.826087</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.270200</td>\n      <td>0.336148</td>\n      <td>0.885000</td>\n      <td>0.886251</td>\n      <td>0.887129</td>\n      <td>0.885375</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.262100</td>\n      <td>0.369545</td>\n      <td>0.890000</td>\n      <td>0.892368</td>\n      <td>0.883721</td>\n      <td>0.901186</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.265700</td>\n      <td>0.353564</td>\n      <td>0.895000</td>\n      <td>0.896755</td>\n      <td>0.892368</td>\n      <td>0.901186</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-500\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1000\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.39420107650756836, metrics={'train_runtime': 260.0432, 'train_samples_per_second': 30.764, 'train_steps_per_second': 3.846, 'total_flos': 0.0, 'train_loss': 0.39420107650756836, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:10.166718Z","iopub.execute_input":"2022-03-27T18:39:10.167233Z","iopub.status.idle":"2022-03-27T18:39:17.086540Z","shell.execute_reply.started":"2022-03-27T18:39:10.167193Z","shell.execute_reply":"2022-03-27T18:39:17.085814Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:06]\n    </div>\n    "},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.3741048574447632,\n 'test_accuracy': 0.885,\n 'test_f1': 0.8827726809378185,\n 'test_precision': 0.8782961460446247,\n 'test_recall': 0.8872950819672131,\n 'test_runtime': 6.9107,\n 'test_samples_per_second': 144.704,\n 'test_steps_per_second': 9.116,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"# 5. Test on 3 comments","metadata":{}},{"cell_type":"markdown","source":"–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ F-score:\n\nSentimentClassifier: 0.8877\n\nSentimentClassifier + CLS: 0.8773\n\nBertForSequenceClassification: 0.8747\n\nSentimentClassifier + multiple CLS: 0.8827\n\n–õ—É—á—à–µ –≤—Å–µ–≥–æ —Å–µ–±—è –ø–æ–∫–∞–∑–∞–ª–∞ –º–æ–¥–µ–ª—å SentimentClassifier, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ CLS —Å–¥–µ–ª–∞–ª–æ —Ç–æ–ª—å–∫–æ —Ö—É–∂–µ. SentimentClassifier + multiple CLS –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –æ–±–µ —Ö–æ—Ä–æ—à–∏\n\n–î–∞–ª–µ–µ –±—É–¥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å BertForSequenceClassification —á—Ç–æ–±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç –≥–æ—Ç–æ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è —Å Huggingface\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Spider-Man: No Way Home 5 stars","metadata":{}},{"cell_type":"code","source":"review = \"\"\"\nBest cinematic experience I've ever had. Cried. Cheered. Cried some more. it's got everything for a Spider-Man fan.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.087649Z","iopub.execute_input":"2022-03-27T18:39:17.087936Z","iopub.status.idle":"2022-03-27T18:39:17.092347Z","shell.execute_reply.started":"2022-03-27T18:39:17.087873Z","shell.execute_reply":"2022-03-27T18:39:17.091411Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"encoding = tokenizer.encode_plus(\n  review,\n  max_length=512,\n  add_special_tokens=True, \n  return_token_type_ids=True,\n  padding='max_length',\n  return_attention_mask=True,\n  return_tensors='pt',  \n  truncation=True\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.093753Z","iopub.execute_input":"2022-03-27T18:39:17.094026Z","iopub.status.idle":"2022-03-27T18:39:17.104048Z","shell.execute_reply.started":"2022-03-27T18:39:17.093991Z","shell.execute_reply":"2022-03-27T18:39:17.103270Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model3(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])[0].cpu().detach().numpy().argmax()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.105692Z","iopub.execute_input":"2022-03-27T18:39:17.105868Z","iopub.status.idle":"2022-03-27T18:39:17.136299Z","shell.execute_reply.started":"2022-03-27T18:39:17.105846Z","shell.execute_reply":"2022-03-27T18:39:17.135551Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"markdown","source":"1 - –∫–ª–∞—Å—Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞, –≤—Å–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ","metadata":{}},{"cell_type":"markdown","source":"## The Wolf of Wall Street 1 star","metadata":{}},{"cell_type":"code","source":"review = \"\"\"\nWorst movie ever I couldn't get more than 1/3 of the way through the movie before having to turn it off. \nContent was horrible, exploiting everybody, foul and DiCaprio not great. It's not even worth 1 star.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.137467Z","iopub.execute_input":"2022-03-27T18:39:17.137694Z","iopub.status.idle":"2022-03-27T18:39:17.141780Z","shell.execute_reply.started":"2022-03-27T18:39:17.137661Z","shell.execute_reply":"2022-03-27T18:39:17.141018Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"encoding = tokenizer.encode_plus(\n  review,\n  max_length=512,\n  add_special_tokens=True, \n  return_token_type_ids=True,\n  padding='max_length',\n  return_attention_mask=True,\n  return_tensors='pt',  \n  truncation=True\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.143198Z","iopub.execute_input":"2022-03-27T18:39:17.143484Z","iopub.status.idle":"2022-03-27T18:39:17.151658Z","shell.execute_reply.started":"2022-03-27T18:39:17.143452Z","shell.execute_reply":"2022-03-27T18:39:17.150922Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"model3(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])[0].cpu().detach().numpy().argmax()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.153246Z","iopub.execute_input":"2022-03-27T18:39:17.153771Z","iopub.status.idle":"2022-03-27T18:39:17.172162Z","shell.execute_reply.started":"2022-03-27T18:39:17.153724Z","shell.execute_reply":"2022-03-27T18:39:17.171535Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"0 - –∫–ª–∞—Å—Å –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ –æ—Ç–∑—ã–≤, —Ç—É—Ç —Ç–æ–∂–µ –≤—Å–µ –≤–µ—Ä–Ω–æ","metadata":{}},{"cell_type":"markdown","source":"## The Matrix Resurrections 3 star","metadata":{}},{"cell_type":"code","source":"review = \"\"\"\nThe trilogy was amazing, but this one falls flat. Was not as excited as the others. Yes, it was romantic, but as a Matrix movie, I expected more action.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.173170Z","iopub.execute_input":"2022-03-27T18:39:17.173405Z","iopub.status.idle":"2022-03-27T18:39:17.178783Z","shell.execute_reply.started":"2022-03-27T18:39:17.173372Z","shell.execute_reply":"2022-03-27T18:39:17.177904Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"encoding = tokenizer.encode_plus(\n  review,\n  max_length=512,\n  add_special_tokens=True, \n  return_token_type_ids=True,\n  padding='max_length',\n  return_attention_mask=True,\n  return_tensors='pt',  \n  truncation=True\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.179901Z","iopub.execute_input":"2022-03-27T18:39:17.180165Z","iopub.status.idle":"2022-03-27T18:39:17.186864Z","shell.execute_reply.started":"2022-03-27T18:39:17.180132Z","shell.execute_reply":"2022-03-27T18:39:17.186145Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model3(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])[0].cpu().detach().numpy().argmax()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.188071Z","iopub.execute_input":"2022-03-27T18:39:17.188364Z","iopub.status.idle":"2022-03-27T18:39:17.205817Z","shell.execute_reply.started":"2022-03-27T18:39:17.188330Z","shell.execute_reply":"2022-03-27T18:39:17.205208Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"–¢—É—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –±—ã–ª–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –ø–æ–≥—Ä–∞–Ω–∏—á–Ω—ã–π —Å–ª—É—á–∞–π –≤ 3 –∑–≤–µ–∑–¥—ã - –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å, –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–µ—Ç –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏. –ú–æ–¥–µ–ª—å —Ä–µ—à–∏–ª–∞, —á—Ç–æ —ç—Ç–æ—Ç—Ç –æ—Ç–∑—ã–≤ —Å–∫–æ—Ä–µ–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π\n\n–í –ø—Ä–∏–Ω—Ü–∏–ø–µ, –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –∏ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–ª–æ—Å—å","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}