{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"# 🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:19:59.817894Z","iopub.execute_input":"2022-03-27T18:19:59.818619Z","iopub.status.idle":"2022-03-27T18:19:59.840803Z","shell.execute_reply.started":"2022-03-27T18:19:59.818531Z","shell.execute_reply":"2022-03-27T18:19:59.840140Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:19:59.842608Z","iopub.execute_input":"2022-03-27T18:19:59.843118Z","iopub.status.idle":"2022-03-27T18:20:08.487352Z","shell.execute_reply.started":"2022-03-27T18:19:59.843080Z","shell.execute_reply":"2022-03-27T18:20:08.486534Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.16.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.3)\nRequirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.11.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.49)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install datasets","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:20:08.488883Z","iopub.execute_input":"2022-03-27T18:20:08.489146Z","iopub.status.idle":"2022-03-27T18:20:16.407641Z","shell.execute_reply.started":"2022-03-27T18:20:08.489110Z","shell.execute_reply":"2022-03-27T18:20:16.406811Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (1.18.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.4)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.62.3)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.4.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.20.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.12.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.11.3)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.26.0)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (5.2.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.2.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\nfrom transformers import BertModel, AutoTokenizer, BertTokenizer, PreTrainedTokenizerFast, AdamW, get_linear_schedule_with_warmup, AutoModelForSequenceClassification\nimport torch.nn.functional as F\nimport torch\nfrom torch.nn import CrossEntropyLoss\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom torch import nn, optim\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.utils.data import Dataset, DataLoader\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nHF_HUB_MODEL = 'prajjwal1/bert-medium'","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:20:16.410266Z","iopub.execute_input":"2022-03-27T18:20:16.410539Z","iopub.status.idle":"2022-03-27T18:20:22.778290Z","shell.execute_reply.started":"2022-03-27T18:20:16.410503Z","shell.execute_reply":"2022-03-27T18:20:22.777511Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 0. Dataset Preparation","metadata":{}},{"cell_type":"markdown","source":"Сделаем шаффл датасета, чтобы у нас были примеры разных классов (вроде как в датасете они идут по поряку). На всякий случай укажем сид, но мы будем использовать они и те же данные для всех моделей","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nimdb_dataset = load_dataset('imdb')\n\nsmall_train_dataset = imdb_dataset[\"train\"].shuffle(seed=42).select(range(5000))\nsmall_test_dataset = imdb_dataset[\"test\"].shuffle(seed=42).select(range(1000))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:20:22.781118Z","iopub.execute_input":"2022-03-27T18:20:22.781375Z","iopub.status.idle":"2022-03-27T18:21:04.955297Z","shell.execute_reply.started":"2022-03-27T18:20:22.781342Z","shell.execute_reply":"2022-03-27T18:21:04.954553Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b676bc10b2294468bf36d07768e31545"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa1159b1fae743bb914713781d71c5a9"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/84.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af683a96516e41f0a4150662d9b68f59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf2d8e3dcbb7472f96b6782e2dbfd9bc"}},"metadata":{}}]},{"cell_type":"markdown","source":"Разделим датасет","metadata":{}},{"cell_type":"code","source":"train_texts = small_train_dataset[\"text\"]\ntrain_labels = small_train_dataset[\"label\"]\ntest_texts = small_test_dataset[\"text\"][:1000]\ntest_labels = small_test_dataset[\"label\"][:1000]\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:04.958322Z","iopub.execute_input":"2022-03-27T18:21:04.958557Z","iopub.status.idle":"2022-03-27T18:21:05.056118Z","shell.execute_reply.started":"2022-03-27T18:21:04.958530Z","shell.execute_reply":"2022-03-27T18:21:05.055280Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(HF_HUB_MODEL)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:05.057512Z","iopub.execute_input":"2022-03-27T18:21:05.057985Z","iopub.status.idle":"2022-03-27T18:21:11.098279Z","shell.execute_reply.started":"2022-03-27T18:21:05.057935Z","shell.execute_reply":"2022-03-27T18:21:11.097259Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/286 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d86386b07d445609ed198ac38cc3d6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e3cfc47eb594093bb6c45728ad63865"}},"metadata":{}}]},{"cell_type":"markdown","source":"Токенизируем датасет используя padding и truncation, так же укажем max_len для последовательности. Наша модель работает с последовательностями дины 512","metadata":{}},{"cell_type":"code","source":"train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_token_type_ids=False, return_attention_mask=True, max_length=512)\nval_encodings = tokenizer(val_texts,truncation=True, padding=True, return_token_type_ids=False, return_attention_mask=True, max_length=512)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, return_token_type_ids=False, return_attention_mask=True, max_length=512)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:11.099533Z","iopub.execute_input":"2022-03-27T18:21:11.099806Z","iopub.status.idle":"2022-03-27T18:21:15.406195Z","shell.execute_reply.started":"2022-03-27T18:21:11.099768Z","shell.execute_reply":"2022-03-27T18:21:15.405430Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class IMDbDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = IMDbDataset(train_encodings, train_labels)\nval_dataset = IMDbDataset(val_encodings, val_labels)\ntest_dataset = IMDbDataset(test_encodings, test_labels)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:15.409157Z","iopub.execute_input":"2022-03-27T18:21:15.409429Z","iopub.status.idle":"2022-03-27T18:21:15.418381Z","shell.execute_reply.started":"2022-03-27T18:21:15.409394Z","shell.execute_reply":"2022-03-27T18:21:15.417655Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Metric","metadata":{}},{"cell_type":"markdown","source":"Будем использовать binary f-score так как у нас бинарная классификация","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:15.419675Z","iopub.execute_input":"2022-03-27T18:21:15.419935Z","iopub.status.idle":"2022-03-27T18:21:15.551051Z","shell.execute_reply.started":"2022-03-27T18:21:15.419898Z","shell.execute_reply":"2022-03-27T18:21:15.550202Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:15.552631Z","iopub.execute_input":"2022-03-27T18:21:15.553218Z","iopub.status.idle":"2022-03-27T18:21:15.560035Z","shell.execute_reply.started":"2022-03-27T18:21:15.553177Z","shell.execute_reply":"2022-03-27T18:21:15.559326Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Training arguments","metadata":{}},{"cell_type":"markdown","source":"Добавим evaluation_strategy = steps, каждые 100 шагов мы будем делать валидацию. Тут также содержатся все гиперпараметры, которые будут использоваться для всех моделей","metadata":{}},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=2,              # total number of training epochs\n    per_device_train_batch_size=8,  # batch size per device during training\n    per_device_eval_batch_size=16,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=100,\n    report_to=None,\n    evaluation_strategy = 'steps'\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:15.561427Z","iopub.execute_input":"2022-03-27T18:21:15.561804Z","iopub.status.idle":"2022-03-27T18:21:15.960356Z","shell.execute_reply.started":"2022-03-27T18:21:15.561767Z","shell.execute_reply":"2022-03-27T18:21:15.959669Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 1. Basic SentimentClassifier ","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"Немного изменим класс SentimentClassifier чтобы засунуть его в Trainer (раньше он не умел работать с labels которые требуются для Trainer)","metadata":{}},{"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(HF_HUB_MODEL)\n        self.n_classes = n_classes\n        self.dropout = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n    def forward( # сорс код из BertModel\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=False,\n    ):\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            return_dict=False)\n        \n        pooled_output = outputs[1]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.out(pooled_output)\n        \n        loss = None # сорс код из BertModel, нужно было сделать кастомный лосс\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.n_classes), labels.view(-1))\n        output = (logits,)\n        return ((loss,) + output) if loss is not None else output","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:15.961722Z","iopub.execute_input":"2022-03-27T18:21:15.961986Z","iopub.status.idle":"2022-03-27T18:21:15.972992Z","shell.execute_reply.started":"2022-03-27T18:21:15.961938Z","shell.execute_reply":"2022-03-27T18:21:15.971166Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"model1 = SentimentClassifier(2) \nmodel1 = model1.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:15.974085Z","iopub.execute_input":"2022-03-27T18:21:15.974260Z","iopub.status.idle":"2022-03-27T18:21:28.242847Z","shell.execute_reply.started":"2022-03-27T18:21:15.974238Z","shell.execute_reply":"2022-03-27T18:21:28.242098Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/159M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c10ef0037e6428ebfc0657029125767"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model1,                         # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset,            # evaluation dataset\n    compute_metrics = compute_metrics    # metrics to evaluate\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:28.244132Z","iopub.execute_input":"2022-03-27T18:21:28.244398Z","iopub.status.idle":"2022-03-27T18:25:46.408386Z","shell.execute_reply.started":"2022-03-27T18:21:28.244364Z","shell.execute_reply":"2022-03-27T18:25:46.407502Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 4000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 04:17, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.693700</td>\n      <td>0.617841</td>\n      <td>0.681000</td>\n      <td>0.728511</td>\n      <td>0.639761</td>\n      <td>0.845850</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.511600</td>\n      <td>0.472773</td>\n      <td>0.785000</td>\n      <td>0.813206</td>\n      <td>0.725581</td>\n      <td>0.924901</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.405700</td>\n      <td>0.416051</td>\n      <td>0.845000</td>\n      <td>0.833154</td>\n      <td>0.914894</td>\n      <td>0.764822</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.417800</td>\n      <td>0.418789</td>\n      <td>0.841000</td>\n      <td>0.845781</td>\n      <td>0.830476</td>\n      <td>0.861660</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.391600</td>\n      <td>0.365908</td>\n      <td>0.840000</td>\n      <td>0.828326</td>\n      <td>0.906103</td>\n      <td>0.762846</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.355200</td>\n      <td>0.616473</td>\n      <td>0.799000</td>\n      <td>0.829517</td>\n      <td>0.726597</td>\n      <td>0.966403</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.311600</td>\n      <td>0.409561</td>\n      <td>0.874000</td>\n      <td>0.876953</td>\n      <td>0.866795</td>\n      <td>0.887352</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.268700</td>\n      <td>0.329048</td>\n      <td>0.882000</td>\n      <td>0.884314</td>\n      <td>0.877432</td>\n      <td>0.891304</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.250600</td>\n      <td>0.359364</td>\n      <td>0.886000</td>\n      <td>0.887795</td>\n      <td>0.884314</td>\n      <td>0.891304</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.270900</td>\n      <td>0.371197</td>\n      <td>0.886000</td>\n      <td>0.888672</td>\n      <td>0.878378</td>\n      <td>0.899209</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-500\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1000\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.38774579620361327, metrics={'train_runtime': 258.0585, 'train_samples_per_second': 31.001, 'train_steps_per_second': 3.875, 'total_flos': 0.0, 'train_loss': 0.38774579620361327, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:25:46.409868Z","iopub.execute_input":"2022-03-27T18:25:46.410246Z","iopub.status.idle":"2022-03-27T18:25:53.292727Z","shell.execute_reply.started":"2022-03-27T18:25:46.410208Z","shell.execute_reply":"2022-03-27T18:25:53.292085Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:06]\n    </div>\n    "},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.3712855279445648,\n 'test_accuracy': 0.89,\n 'test_f1': 0.8877551020408164,\n 'test_precision': 0.8841463414634146,\n 'test_recall': 0.8913934426229508,\n 'test_runtime': 6.8724,\n 'test_samples_per_second': 145.509,\n 'test_steps_per_second': 9.167,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"# 2. SentimentClassifier with CLS token","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"В предыдущий класс добавим CLS токен. CLS токен это первый токен из last_hidden_state","metadata":{}},{"cell_type":"code","source":"class CLSSentimentClassifier(nn.Module):\n    \n    def __init__(self, n_classes):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(HF_HUB_MODEL)\n        self.n_classes = n_classes\n        self.dropout = nn.Dropout(p=0.3)\n        self.linear = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n        self.out = nn.Linear(self.bert.config.hidden_size + 512, n_classes) # так как добавляем CLS токен, надо расширить размерносить выхода на hidden_size\n        \n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=False,\n    ):\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            return_dict=False)\n        \n\n        CLS_token = outputs[0][:,0,:] # CLS это первый токен last_hidden_state\n        pooled_output = self.linear(self.dropout(outputs[1])) \n        stacked = torch.hstack([CLS_token, pooled_output]) # склеим CLS token и pooled output\n\n        logits = self.out(stacked)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.n_classes), labels.view(-1))\n        output = (logits,)\n        return ((loss,) + output) if loss is not None else output","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:25:53.294207Z","iopub.execute_input":"2022-03-27T18:25:53.294484Z","iopub.status.idle":"2022-03-27T18:25:53.304537Z","shell.execute_reply.started":"2022-03-27T18:25:53.294448Z","shell.execute_reply":"2022-03-27T18:25:53.303812Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"model2 = CLSSentimentClassifier(2) \nmodel2 = model2.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:25:53.305927Z","iopub.execute_input":"2022-03-27T18:25:53.306428Z","iopub.status.idle":"2022-03-27T18:25:54.847099Z","shell.execute_reply.started":"2022-03-27T18:25:53.306389Z","shell.execute_reply":"2022-03-27T18:25:54.846386Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/prajjwal1/bert-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/288b0ee1e79a7c3fe770ab8a84ece013c573e7d226ccb5d9ffad317b3419faac.4344f82f77799c092b30b2e0d3749c809f82df14c5993e43dbbdc52f5a0d86e0\nModel config BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 512,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2048,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 8,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/prajjwal1/bert-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/dabb6f3bc29449f038f41cb09eb1a693eee2bee3dab8afff878a2910fa73a171.b722b1c13187b9ed20e5e36ab761041218e88d502895424e3ed2516bc9693089\nSome weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of BertModel were initialized from the model checkpoint at prajjwal1/bert-medium.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model2,                         # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset,            # evaluation dataset\n    compute_metrics = compute_metrics    # metrics to evaluate\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:25:54.848406Z","iopub.execute_input":"2022-03-27T18:25:54.848807Z","iopub.status.idle":"2022-03-27T18:30:14.677918Z","shell.execute_reply.started":"2022-03-27T18:25:54.848682Z","shell.execute_reply":"2022-03-27T18:30:14.677165Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 4000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 04:19, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.685500</td>\n      <td>0.585503</td>\n      <td>0.725000</td>\n      <td>0.744186</td>\n      <td>0.702988</td>\n      <td>0.790514</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.485100</td>\n      <td>0.441349</td>\n      <td>0.824000</td>\n      <td>0.839416</td>\n      <td>0.779661</td>\n      <td>0.909091</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.410100</td>\n      <td>0.431481</td>\n      <td>0.835000</td>\n      <td>0.844193</td>\n      <td>0.808318</td>\n      <td>0.883399</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.443000</td>\n      <td>0.433221</td>\n      <td>0.851000</td>\n      <td>0.859301</td>\n      <td>0.822785</td>\n      <td>0.899209</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.403900</td>\n      <td>0.327260</td>\n      <td>0.858000</td>\n      <td>0.859684</td>\n      <td>0.859684</td>\n      <td>0.859684</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.314100</td>\n      <td>0.424729</td>\n      <td>0.851000</td>\n      <td>0.860878</td>\n      <td>0.815929</td>\n      <td>0.911067</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.342900</td>\n      <td>0.387718</td>\n      <td>0.867000</td>\n      <td>0.865521</td>\n      <td>0.886128</td>\n      <td>0.845850</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.256000</td>\n      <td>0.334733</td>\n      <td>0.877000</td>\n      <td>0.873065</td>\n      <td>0.913607</td>\n      <td>0.835968</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.255200</td>\n      <td>0.405889</td>\n      <td>0.885000</td>\n      <td>0.888889</td>\n      <td>0.869565</td>\n      <td>0.909091</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.263200</td>\n      <td>0.395804</td>\n      <td>0.888000</td>\n      <td>0.888668</td>\n      <td>0.894000</td>\n      <td>0.883399</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-500\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1000\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.38589393615722656, metrics={'train_runtime': 259.7948, 'train_samples_per_second': 30.794, 'train_steps_per_second': 3.849, 'total_flos': 0.0, 'train_loss': 0.38589393615722656, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:30:14.679539Z","iopub.execute_input":"2022-03-27T18:30:14.684156Z","iopub.status.idle":"2022-03-27T18:30:21.594479Z","shell.execute_reply.started":"2022-03-27T18:30:14.684112Z","shell.execute_reply":"2022-03-27T18:30:21.593810Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:06]\n    </div>\n    "},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.3970975875854492,\n 'test_accuracy': 0.882,\n 'test_f1': 0.8773388773388774,\n 'test_precision': 0.890295358649789,\n 'test_recall': 0.8647540983606558,\n 'test_runtime': 6.9012,\n 'test_samples_per_second': 144.902,\n 'test_steps_per_second': 9.129,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"# 3. Transformers - BertForSequenceClassification","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"Тут ничего сложного: возьмем BertForSequenceClassification и загрузим предобученную модель с Huggingface","metadata":{}},{"cell_type":"code","source":"from transformers import  BertForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:30:21.595805Z","iopub.execute_input":"2022-03-27T18:30:21.596094Z","iopub.status.idle":"2022-03-27T18:30:21.601417Z","shell.execute_reply.started":"2022-03-27T18:30:21.596058Z","shell.execute_reply":"2022-03-27T18:30:21.600645Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"model3 = BertForSequenceClassification.from_pretrained(HF_HUB_MODEL)\nmodel3 = model3.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:30:21.603513Z","iopub.execute_input":"2022-03-27T18:30:21.604218Z","iopub.status.idle":"2022-03-27T18:30:23.092874Z","shell.execute_reply.started":"2022-03-27T18:30:21.604181Z","shell.execute_reply":"2022-03-27T18:30:23.092123Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/prajjwal1/bert-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/288b0ee1e79a7c3fe770ab8a84ece013c573e7d226ccb5d9ffad317b3419faac.4344f82f77799c092b30b2e0d3749c809f82df14c5993e43dbbdc52f5a0d86e0\nModel config BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 512,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2048,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 8,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/prajjwal1/bert-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/dabb6f3bc29449f038f41cb09eb1a693eee2bee3dab8afff878a2910fa73a171.b722b1c13187b9ed20e5e36ab761041218e88d502895424e3ed2516bc9693089\nSome weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model3,                         # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset,            # evaluation dataset\n    compute_metrics = compute_metrics    # metrics to evaluate\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:30:23.094302Z","iopub.execute_input":"2022-03-27T18:30:23.094570Z","iopub.status.idle":"2022-03-27T18:34:41.744428Z","shell.execute_reply.started":"2022-03-27T18:30:23.094535Z","shell.execute_reply":"2022-03-27T18:34:41.742773Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 4000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 04:18, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.679300</td>\n      <td>0.618526</td>\n      <td>0.735000</td>\n      <td>0.749764</td>\n      <td>0.717902</td>\n      <td>0.784585</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.497700</td>\n      <td>0.349631</td>\n      <td>0.861000</td>\n      <td>0.862784</td>\n      <td>0.861933</td>\n      <td>0.863636</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.394300</td>\n      <td>0.403675</td>\n      <td>0.840000</td>\n      <td>0.851577</td>\n      <td>0.802448</td>\n      <td>0.907115</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.379700</td>\n      <td>0.389311</td>\n      <td>0.850000</td>\n      <td>0.855769</td>\n      <td>0.833333</td>\n      <td>0.879447</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.392800</td>\n      <td>0.374978</td>\n      <td>0.844000</td>\n      <td>0.832976</td>\n      <td>0.908879</td>\n      <td>0.768775</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.348000</td>\n      <td>0.434627</td>\n      <td>0.857000</td>\n      <td>0.871287</td>\n      <td>0.800000</td>\n      <td>0.956522</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.327500</td>\n      <td>0.421971</td>\n      <td>0.855000</td>\n      <td>0.844920</td>\n      <td>0.920746</td>\n      <td>0.780632</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.292900</td>\n      <td>0.295128</td>\n      <td>0.891000</td>\n      <td>0.893451</td>\n      <td>0.883946</td>\n      <td>0.903162</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.229800</td>\n      <td>0.322991</td>\n      <td>0.890000</td>\n      <td>0.894636</td>\n      <td>0.868030</td>\n      <td>0.922925</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.262500</td>\n      <td>0.324668</td>\n      <td>0.895000</td>\n      <td>0.897361</td>\n      <td>0.887814</td>\n      <td>0.907115</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-500\nConfiguration saved in ./results/checkpoint-500/config.json\nModel weights saved in ./results/checkpoint-500/pytorch_model.bin\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1000\nConfiguration saved in ./results/checkpoint-1000/config.json\nModel weights saved in ./results/checkpoint-1000/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.3804571762084961, metrics={'train_runtime': 258.609, 'train_samples_per_second': 30.935, 'train_steps_per_second': 3.867, 'total_flos': 626289328128000.0, 'train_loss': 0.3804571762084961, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:34:41.750808Z","iopub.execute_input":"2022-03-27T18:34:41.753069Z","iopub.status.idle":"2022-03-27T18:34:48.606617Z","shell.execute_reply.started":"2022-03-27T18:34:41.753030Z","shell.execute_reply":"2022-03-27T18:34:48.605920Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:06]\n    </div>\n    "},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.38930660486221313,\n 'test_accuracy': 0.878,\n 'test_f1': 0.8747433264887063,\n 'test_precision': 0.8765432098765432,\n 'test_recall': 0.8729508196721312,\n 'test_runtime': 6.8438,\n 'test_samples_per_second': 146.118,\n 'test_steps_per_second': 9.205,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"# 4. SentimentClassifier with aggregated CLS-tokens from layers","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"Для того чтобы аггрегировать CLS-токены для нескольких слоев, сделаем mean pooling по слою hidden_state","metadata":{}},{"cell_type":"code","source":"class AGGCLSSentimentClassifier(nn.Module):\n    \n    def __init__(self, n_classes):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(HF_HUB_MODEL)\n        self.n_classes = n_classes\n        self.dropout = nn.Dropout(p=0.3)\n        self.linear = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n        self.out = nn.Linear(self.bert.config.hidden_size + 512, n_classes) \n        \n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=False,\n    ):\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            output_hidden_states=True,\n                            return_dict=False)\n        \n        output_hidden_states = outputs[2]\n        \n        hidden_states = torch.stack(output_hidden_states) # берем слои hidden_state слои\n        \n        CLS_tokens = torch.mean(hidden_states[:, :, 0], 0) # делаем mean pooling\n        pooled_output = self.linear(self.dropout(outputs[1]))\n        stacked = torch.hstack([CLS_tokens, pooled_output]) # соединяем mean pooling и pooled output\n    \n\n        logits = self.out(stacked)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.n_classes), labels.view(-1))\n        output = (logits,)\n        return ((loss,) + output) if loss is not None else output","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:34:48.607713Z","iopub.execute_input":"2022-03-27T18:34:48.607962Z","iopub.status.idle":"2022-03-27T18:34:48.618921Z","shell.execute_reply.started":"2022-03-27T18:34:48.607926Z","shell.execute_reply":"2022-03-27T18:34:48.618277Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"model4 = AGGCLSSentimentClassifier(2) \nmodel4 = model4.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:34:48.620235Z","iopub.execute_input":"2022-03-27T18:34:48.620660Z","iopub.status.idle":"2022-03-27T18:34:50.092448Z","shell.execute_reply.started":"2022-03-27T18:34:48.620623Z","shell.execute_reply":"2022-03-27T18:34:50.091681Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/prajjwal1/bert-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/288b0ee1e79a7c3fe770ab8a84ece013c573e7d226ccb5d9ffad317b3419faac.4344f82f77799c092b30b2e0d3749c809f82df14c5993e43dbbdc52f5a0d86e0\nModel config BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 512,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2048,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 8,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/prajjwal1/bert-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/dabb6f3bc29449f038f41cb09eb1a693eee2bee3dab8afff878a2910fa73a171.b722b1c13187b9ed20e5e36ab761041218e88d502895424e3ed2516bc9693089\nSome weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of BertModel were initialized from the model checkpoint at prajjwal1/bert-medium.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model4,                         # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset,            # evaluation dataset\n    compute_metrics = compute_metrics    # metrics to evaluate\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:34:50.093626Z","iopub.execute_input":"2022-03-27T18:34:50.094421Z","iopub.status.idle":"2022-03-27T18:39:10.165241Z","shell.execute_reply.started":"2022-03-27T18:34:50.094378Z","shell.execute_reply":"2022-03-27T18:39:10.164483Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 4000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 04:19, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.707200</td>\n      <td>0.647586</td>\n      <td>0.679000</td>\n      <td>0.658147</td>\n      <td>0.713626</td>\n      <td>0.610672</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.532500</td>\n      <td>0.388303</td>\n      <td>0.839000</td>\n      <td>0.842002</td>\n      <td>0.836257</td>\n      <td>0.847826</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.400700</td>\n      <td>0.432419</td>\n      <td>0.849000</td>\n      <td>0.850643</td>\n      <td>0.851485</td>\n      <td>0.849802</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.430600</td>\n      <td>0.525616</td>\n      <td>0.831000</td>\n      <td>0.848158</td>\n      <td>0.777595</td>\n      <td>0.932806</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.400000</td>\n      <td>0.347810</td>\n      <td>0.861000</td>\n      <td>0.857436</td>\n      <td>0.891258</td>\n      <td>0.826087</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.361200</td>\n      <td>0.434164</td>\n      <td>0.847000</td>\n      <td>0.863026</td>\n      <td>0.788871</td>\n      <td>0.952569</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.311800</td>\n      <td>0.376082</td>\n      <td>0.881000</td>\n      <td>0.875393</td>\n      <td>0.930958</td>\n      <td>0.826087</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.270200</td>\n      <td>0.336148</td>\n      <td>0.885000</td>\n      <td>0.886251</td>\n      <td>0.887129</td>\n      <td>0.885375</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.262100</td>\n      <td>0.369545</td>\n      <td>0.890000</td>\n      <td>0.892368</td>\n      <td>0.883721</td>\n      <td>0.901186</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.265700</td>\n      <td>0.353564</td>\n      <td>0.895000</td>\n      <td>0.896755</td>\n      <td>0.892368</td>\n      <td>0.901186</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-500\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1000\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.39420107650756836, metrics={'train_runtime': 260.0432, 'train_samples_per_second': 30.764, 'train_steps_per_second': 3.846, 'total_flos': 0.0, 'train_loss': 0.39420107650756836, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:10.166718Z","iopub.execute_input":"2022-03-27T18:39:10.167233Z","iopub.status.idle":"2022-03-27T18:39:17.086540Z","shell.execute_reply.started":"2022-03-27T18:39:10.167193Z","shell.execute_reply":"2022-03-27T18:39:17.085814Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:06]\n    </div>\n    "},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.3741048574447632,\n 'test_accuracy': 0.885,\n 'test_f1': 0.8827726809378185,\n 'test_precision': 0.8782961460446247,\n 'test_recall': 0.8872950819672131,\n 'test_runtime': 6.9107,\n 'test_samples_per_second': 144.704,\n 'test_steps_per_second': 9.116,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"# 5. Test on 3 comments","metadata":{}},{"cell_type":"markdown","source":"Посмотрим на F-score:\n\nSentimentClassifier: 0.8877\n\nSentimentClassifier + CLS: 0.8773\n\nBertForSequenceClassification: 0.8747\n\nSentimentClassifier + multiple CLS: 0.8827\n\nЛучше всего себя показала модель SentimentClassifier, добавление CLS сделало только хуже. SentimentClassifier + multiple CLS показывают примерно одинаковый результат, обе хороши\n\nДалее буду использовать BertForSequenceClassification чтобы посмотреть, как работают готовые решения с Huggingface\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Spider-Man: No Way Home 5 stars","metadata":{}},{"cell_type":"code","source":"review = \"\"\"\nBest cinematic experience I've ever had. Cried. Cheered. Cried some more. it's got everything for a Spider-Man fan.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.087649Z","iopub.execute_input":"2022-03-27T18:39:17.087936Z","iopub.status.idle":"2022-03-27T18:39:17.092347Z","shell.execute_reply.started":"2022-03-27T18:39:17.087873Z","shell.execute_reply":"2022-03-27T18:39:17.091411Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"encoding = tokenizer.encode_plus(\n  review,\n  max_length=512,\n  add_special_tokens=True, \n  return_token_type_ids=True,\n  padding='max_length',\n  return_attention_mask=True,\n  return_tensors='pt',  \n  truncation=True\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.093753Z","iopub.execute_input":"2022-03-27T18:39:17.094026Z","iopub.status.idle":"2022-03-27T18:39:17.104048Z","shell.execute_reply.started":"2022-03-27T18:39:17.093991Z","shell.execute_reply":"2022-03-27T18:39:17.103270Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model3(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])[0].cpu().detach().numpy().argmax()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.105692Z","iopub.execute_input":"2022-03-27T18:39:17.105868Z","iopub.status.idle":"2022-03-27T18:39:17.136299Z","shell.execute_reply.started":"2022-03-27T18:39:17.105846Z","shell.execute_reply":"2022-03-27T18:39:17.135551Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"markdown","source":"1 - класс положительного отзыва, все правильно","metadata":{}},{"cell_type":"markdown","source":"## The Wolf of Wall Street 1 star","metadata":{}},{"cell_type":"code","source":"review = \"\"\"\nWorst movie ever I couldn't get more than 1/3 of the way through the movie before having to turn it off. \nContent was horrible, exploiting everybody, foul and DiCaprio not great. It's not even worth 1 star.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.137467Z","iopub.execute_input":"2022-03-27T18:39:17.137694Z","iopub.status.idle":"2022-03-27T18:39:17.141780Z","shell.execute_reply.started":"2022-03-27T18:39:17.137661Z","shell.execute_reply":"2022-03-27T18:39:17.141018Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"encoding = tokenizer.encode_plus(\n  review,\n  max_length=512,\n  add_special_tokens=True, \n  return_token_type_ids=True,\n  padding='max_length',\n  return_attention_mask=True,\n  return_tensors='pt',  \n  truncation=True\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.143198Z","iopub.execute_input":"2022-03-27T18:39:17.143484Z","iopub.status.idle":"2022-03-27T18:39:17.151658Z","shell.execute_reply.started":"2022-03-27T18:39:17.143452Z","shell.execute_reply":"2022-03-27T18:39:17.150922Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"model3(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])[0].cpu().detach().numpy().argmax()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.153246Z","iopub.execute_input":"2022-03-27T18:39:17.153771Z","iopub.status.idle":"2022-03-27T18:39:17.172162Z","shell.execute_reply.started":"2022-03-27T18:39:17.153724Z","shell.execute_reply":"2022-03-27T18:39:17.171535Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"0 - класс негативного отзыв, тут тоже все верно","metadata":{}},{"cell_type":"markdown","source":"## The Matrix Resurrections 3 star","metadata":{}},{"cell_type":"code","source":"review = \"\"\"\nThe trilogy was amazing, but this one falls flat. Was not as excited as the others. Yes, it was romantic, but as a Matrix movie, I expected more action.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.173170Z","iopub.execute_input":"2022-03-27T18:39:17.173405Z","iopub.status.idle":"2022-03-27T18:39:17.178783Z","shell.execute_reply.started":"2022-03-27T18:39:17.173372Z","shell.execute_reply":"2022-03-27T18:39:17.177904Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"encoding = tokenizer.encode_plus(\n  review,\n  max_length=512,\n  add_special_tokens=True, \n  return_token_type_ids=True,\n  padding='max_length',\n  return_attention_mask=True,\n  return_tensors='pt',  \n  truncation=True\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.179901Z","iopub.execute_input":"2022-03-27T18:39:17.180165Z","iopub.status.idle":"2022-03-27T18:39:17.186864Z","shell.execute_reply.started":"2022-03-27T18:39:17.180132Z","shell.execute_reply":"2022-03-27T18:39:17.186145Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model3(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])[0].cpu().detach().numpy().argmax()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:39:17.188071Z","iopub.execute_input":"2022-03-27T18:39:17.188364Z","iopub.status.idle":"2022-03-27T18:39:17.205817Z","shell.execute_reply.started":"2022-03-27T18:39:17.188330Z","shell.execute_reply":"2022-03-27T18:39:17.205208Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"Тут интересно было посмотреть на пограничный случай в 3 звезды - нейтральный класс, которого нет при обучении. Модель решила, что этотт отзыв скорее негативный\n\nВ принципе, модель работает как и предполагалось","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}